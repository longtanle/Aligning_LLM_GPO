{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RLHF with DPO & Hugging Face"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is run on Python 3.10.13 and Pytorch 2.1.2+cu118. \n",
    "\n",
    "Our first step is to install Hugging Face Libraries including trl, transformers and datasets. TRL is a new library on top of transformers and datasets, which makes it easier to fine-tune, rlhf, align open LLMs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "# Install Hugging Face libraries\n",
    "!pip install  --upgrade \\\n",
    "  \"transformers[sentencepiece]==4.37.2\" \\\n",
    "  \"datasets==2.16.1\" \\\n",
    "  \"accelerate==0.26.1\" \\\n",
    "  \"evaluate==0.4.1\" \\\n",
    "  \"bitsandbytes==0.42.0\" \\\n",
    "  \"trl==0.7.11\" \\\n",
    "  \"peft==0.8.2\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are using a GPU with Ampere architecture (e.g. NVIDIA A10G or RTX 4090/3090) or newer, you can use [Flash Attention](https://github.com/Dao-AILab/flash-attention/tree/main), which help accelerate training time up to 3x.\n",
    "\n",
    "``Note``: If your machine has less than 96GB of RAM and lots of CPU cores, reduce the number of ```MAX_JOBS```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "import torch; assert torch.cuda.get_device_capability()[0] >= 8, 'Hardware not supported for Flash Attention'\n",
    "# install flash-attn\n",
    "!pip install ninja packaging\n",
    "!MAX_JOBS=4 pip install flash-attn --no-build-isolation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the Hugging Face Hub as a remote model storage and automatically push our model, logs and information to the Hub during training. You must register on the Hugging Face for this. After you have an account, we will use the login util from the huggingface_hub package to log into our account and store our token (access key) on the disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "\n",
    "login(\n",
    "  token=\"\", # ADD YOUR TOKEN HERE\n",
    "  add_to_git_credential=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Create and prepare the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Improving the helpfulness or quality of LLMs through Aligning methods like DPO doesn’t come for free. Compared to traditional supervised fine-tuning (SFT) alignment methods require preference data, serving as a proxy against which the model's outputs are evaluated and aligned. A typical DPO dataset includes a triplet out of ```prompt```, ``chosen``, and ``rejected`` response. There are several ways to create such a dataset, including:\n",
    "\n",
    "- Using existing open-source datasets, e.g., [SHP](https://huggingface.co/datasets/stanfordnlp/SHP)\n",
    "- Using LLMs to create synthetic preferences, e.g., [Ultrafeedback](https://www.notion.so/9de9ac96f0f94aa5aed96361a26e8bf0?pvs=21)\n",
    "- Using Humans to create datasets, e.g., [HH](https://www.notion.so/SageMaker-bi-weekly-sync-0be2e6ba876a4599b4c0da2681dfb78f?pvs=21)\n",
    "- Using a combination of the above methods, e.g., [Orca DPO](https://huggingface.co/datasets/Intel/orca_dpo_pairs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our example, we will use the [argilla/ultrafeedback-binarized-preferences-cleaned](https://huggingface.co/datasets/argilla/ultrafeedback-binarized-preferences-cleaned?row=0) dataset, an well-known DPO dataset representing the real-world preferences of human. \n",
    "\n",
    "If you don’t have collected preferences yet, start with your existing SFT data and use different sizes/quality LLMs to generate feedback. This method was used to create the Orca DPO dataset, where GPT-4 was used for the accepted responses and Llama 70B Chat for the rejected responses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A DPO dataset will have the following format\n",
    "```\n",
    "{\"chosen\": \"<prompt + good response>\", \"rejected\": \"<prompt + worse response>\" }\n",
    "{\"chosen\": \"<prompt + good response>\", \"rejected\": \"<prompt + worse response>\" }\n",
    "{\"chosen\": \"<prompt + good response>\", \"rejected\": \"<prompt + worse response>\" }\n",
    "```\n",
    "\n",
    "The `<pompt + good response>` and `<prompt + worse response>` are representend in the conversational format as:\n",
    "```\n",
    "[\n",
    "  { \"role\": \"system\", \"content\": \"You are...\" },\n",
    "  { \"role\": \"user\", \"content\": \"...\" },\n",
    "  { \"role\": \"assistant\", \"content\": \"...\" }\n",
    "]\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Note`: If the dataset includes multiple turns you need to make sure that only the last turn between chosen and rejected is different. If not, you must reduce the conversation until only the last assistant turn is different."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The DPOTrainer expects the inputs as triples of ```(prompt, chosen, rejected)```, where ```chosen``` and ```rejected``` are the final turn of a dialogue and the prompt is N-1 turns. Those inputs also need to be already formated with the tempalte of the model, e.g. ```<|im_start|>user\\nINSTRUCTION\\n<|im_end|>\\n<|im_start|>assistant\\n....```\n",
    "\n",
    "In our example we are going to load our open-source dataset using the HuggingFace Datasets library and then convert it into the correct format. The argilla/ultrafeedback-binarized-preferences-cleaned already comes with the DPO format (chosen/rejected). This means we can create our triplet and templetite it using a tokenizer and the apply_chat_template methoh. We are randomly downsampling the dataset to 11,000 training samples and 2,750 test samples\n",
    "\n",
    "*Note*: This step can be different for your use case. For example, if you might need to create the conversational format and concate the prompt and ```chosen/rejected``` response, e.g. ```Human:\\n ... Assistant:\\n.```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lole4878/miniconda3/envs/torch212/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Map: 100%|██████████| 13750/13750 [00:02<00:00, 5458.51 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|system|>\n",
      "You are Dolphin, a helpful AI assistant\n",
      "<|assistant|>\n",
      "Protein domains are distinct, self-c\n",
      "<|assistant|>\n",
      "Of course, I'd be happy to help! To \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating json from Arrow format: 100%|██████████| 11/11 [00:00<00:00, 38.13ba/s]\n",
      "Creating json from Arrow format: 100%|██████████| 3/3 [00:00<00:00, 42.82ba/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "9571226"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Load Tokenizer from the hub\n",
    "#model_id = \"cognitivecomputations/dolphin-2.1-mistral-7b\" # replace with your model id\n",
    "model_id = \"alignment-handbook/zephyr-7b-sft-qlora\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "# Load dataset from the hub\n",
    "dataset = load_dataset(\"argilla/ultrafeedback-binarized-preferences-cleaned\",\n",
    "                        split=\"train\")\n",
    "dataset = dataset.shuffle().select(range(13750))\n",
    "\n",
    "\n",
    "def rec_extract_assistant_messages(messages, index=-1):\n",
    "  \"\"\"Recursively extract the last assistant messages from the end of the conversation.\"\"\"\n",
    "  if messages[index][\"role\"] == \"assistant\":\n",
    "    return [messages[index]]\n",
    "  else:\n",
    "    return rec_extract_assistant_messages(messages, index-1)\n",
    "\n",
    "# System message used if there is no system message at the beginning of the conversation\n",
    "# Can be repelaced and modified as needed\n",
    "DEFAULT_SYSTEM_MESSAGE = \"You are Dolphin, a helpful AI assistant.\"\n",
    "\n",
    "def create_triplets(example, tokenizer, default_system_message=DEFAULT_SYSTEM_MESSAGE):\n",
    "  \"\"\"Create the triplets (prompt, chosen, rejected)\"\"\"\n",
    "  # Extract the N-1 turns to form the prompt\n",
    "  # Prepend a system message if the first message is not a system message\n",
    "  prompt_messages = example[\"chosen\"][:-1]\n",
    "  if example[\"chosen\"][0][\"role\"] != \"system\":\n",
    "      prompt_messages.insert(0, {\"role\": \"system\", \"content\": default_system_message})\n",
    "  # Now we extract the final assistant turn to define chosen/rejected responses\n",
    "  chosen_messages = rec_extract_assistant_messages(example[\"chosen\"])\n",
    "  rejected_messages = rec_extract_assistant_messages(example[\"rejected\"])\n",
    "\n",
    "  # apply template to the messages and return the triplets\n",
    "  return {\n",
    "    \"prompt\": tokenizer.apply_chat_template(prompt_messages, tokenize=False),\n",
    "    \"chosen\": tokenizer.apply_chat_template(chosen_messages, tokenize=False),\n",
    "    \"rejected\": tokenizer.apply_chat_template(rejected_messages, tokenize=False)\n",
    "  }\n",
    "\n",
    "dataset = dataset.map(create_triplets,\n",
    "                      remove_columns=dataset.features,\n",
    "                      fn_kwargs={\"tokenizer\": tokenizer})\n",
    "# split dataset into 11,000 training samples and 2,750 test samples\n",
    "dataset = dataset.train_test_split(test_size=2750/13750)\n",
    "\n",
    "# print sample cut of\n",
    "print(dataset[\"train\"][0][\"prompt\"][:50])\n",
    "print(dataset[\"train\"][0][\"chosen\"][:50])\n",
    "print(dataset[\"train\"][0][\"rejected\"][:50])\n",
    "\n",
    "# save datasets to disk\n",
    "dataset[\"train\"].to_json(\"train_dataset.json\", orient=\"records\")\n",
    "dataset[\"test\"].to_json(\"test_dataset.json\", orient=\"records\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Align LLM with TRL and the DPOTrainer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TRL supports the DPO through a dedicated [DPOTrainer](https://huggingface.co/docs/trl/dpo_trainer) for alinging LLMs from preference data, as described in **Direct Preference Optimization: Your Language Model is Secretly a Reward Model**. The DPOTrainer is a subclass of the Trainer from the transformers library and supports all the same features, including logging, evaluation, and checkpointing.\n",
    "\n",
    "*One big difference to SFT is that for DPO we need an additional Reference Model, which is used for KL-Divergence to help stabilize the training*. The Reference Model is normally the same model as the one we are training, but frozen. This means for DPO you need additional memory and compute resources. To keep our example efficient we will use PEFT and adatpers. We load your fine-tuned and then add a new trainable adapters. This means that we will only tune adapters and not the whole model using DPO. The original model will be then used as reference model itself. If you want to train all parameter with DPO you need to provide a `model` and `reference_model`, but this requires more memory and compute resources.\n",
    "\n",
    "Lets start by loading our saved datasets from disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Load jsonl data from disk\n",
    "train_dataset = load_dataset(\"json\", data_files=\"train_dataset.json\", split=\"train\")\n",
    "eval_dataset = load_dataset(\"json\", data_files=\"test_dataset.json\", split=\"train\")\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to train ``alignment-handbook/zephyr-7b-sft-qlora``. Zephyr-7b is a fine-tuned Mistral 7B with ChatML template support system messages. You can easily swap out the model for another model, e.g. Mistral or Mixtral models, TII Falcon, or any other LLMs by changing our model_id variable.\n",
    "\n",
    "*Note*: Be aware the bigger the model the more memory it will require. In our example we will use the 7B version, which can be tuned on 24GB GPUs. If you have a smaller GPU."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n",
      "Downloading shards: 100%|██████████| 2/2 [00:00<00:00,  4.30it/s]\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.99s/it]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "from accelerate import Accelerator\n",
    "\n",
    "# Hugging Face model id\n",
    "model_id = \"alignment-handbook/zephyr-7b-sft-qlora\" # replace with your model id\n",
    "\n",
    "# BitsAndBytesConfig int-4 config\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "# Load model and tokenizer\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    #device_map=\"auto\",\n",
    "    device_map={\"\": Accelerator().local_process_index},\n",
    "    use_cache=False,\n",
    "    attn_implementation=\"flash_attention_2\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    quantization_config=bnb_config\n",
    ")\n",
    "\n",
    "model.gradient_checkpointing_enable() #gradient checkpointing to save memory\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = 'left' # to prevent errors with FA\n",
    "tokenizer.truncation_side = 'left' # to prevent cutting off last generation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compared to the `SFTTrainer` the `DPOTrainer` has two parameter related to dataset sizing:\n",
    "- `max_prompt_length`: the maximum length of the prompt and \n",
    "- `max_length`: the maximum length of the prompt + chosen or rejected response. \n",
    "\n",
    "Those are used for tokenization, padding and trunctation. This means if we set those wrongly our data will be potentially cut off, but if we set them too high we will waste memory and time.\n",
    "\n",
    "The [Alignment Handbook](https://github.com/huggingface/alignment-handbook) when with the `max_prompt_length` of 512 and `max_length` of 1024 combining it with the truncation side left (90% of data samples where in that range). Truncation side left means the beginning will be removed so we keep the important assistant response. In our example we want to cover the ~97% percentile and filter out longer samples, rather than truncating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### COMMENT IN TO RECALCULATE MAX LENGTHS ####\n",
    "# from numpy import percentile\n",
    "\n",
    "# # lets find the p95 length of the prompt\n",
    "# prompt_length = int(percentile([len(tokenizer(x)[\"input_ids\"]) for x in train_dataset[\"prompt\"]], 95))\n",
    "# max_seq_length_chosen = int(percentile([len(tokenizer(x[\"prompt\"] + x[\"chosen\"])[\"input_ids\"]) for x in train_dataset], 95))\n",
    "# max_seq_length_rejected = int(percentile([len(tokenizer(x[\"prompt\"] + x[\"rejected\"])[\"input_ids\"]) for x in train_dataset], 95))\n",
    "# max_seq_length = max(max_seq_length_chosen, max_seq_length_rejected)\n",
    "\n",
    "# # filter datasets to remove samples that are too long\n",
    "# train_dataset = train_dataset.filter(lambda x: len(tokenizer(x[\"prompt\"] + x[\"chosen\"])[\"input_ids\"]) <= max_seq_length)\n",
    "# eval_dataset = eval_dataset.filter(lambda x: len(tokenizer(x[\"prompt\"] + x[\"chosen\"])[\"input_ids\"]) <= max_seq_length)\n",
    "# print(f\"len(train_dataset): {len(train_dataset)}\")\n",
    "# print(f\"len(eval_dataset): {len(eval_dataset)}\")\n",
    "\n",
    "# # Up the lengths to next multiple of 2, why 2? Don't know\n",
    "# prompt_length = ((prompt_length + 1) // 2) * 2\n",
    "# max_seq_length = ((max_seq_length + 1) // 2) * 2\n",
    "# print(f\"p95 prompt length: {prompt_length}\")\n",
    "# print(f\"p95 prompt + chosen length: {max_seq_length}\")\n",
    "\n",
    "prompt_length = 512\n",
    "max_seq_length = 1024"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The DPOTrainer supports a native integration with peft, which makes it super easy to efficiently align LLMs using, e.g. QLoRA. We only need to create our LoraConfig and provide it to the trainer. Our LoraConfig parameters are the same as for the SFT example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig\n",
    "\n",
    "# LoRA config based on QLoRA paper & Sebastian Raschka experiment\n",
    "peft_config = LoraConfig(\n",
    "        lora_alpha=16,\n",
    "        lora_dropout=0.05,\n",
    "        r=16,\n",
    "        bias=\"none\",\n",
    "        #target_modules=\"all-linear\",\n",
    "        #target_modules=['k_proj','v_proj', 'q_proj', 'dense'],\n",
    "        target_modules=['k_proj',\n",
    "                        'gate_proj',\n",
    "                        'v_proj',\n",
    "                        'up_proj',\n",
    "                        'q_proj',\n",
    "                        'o_proj',\n",
    "                        'down_proj'],\n",
    "        task_type=\"CAUSAL_LM\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we can start our training we need to define the hyperparameters (`TrainingArguments`) & DPO parameters.\n",
    "\n",
    "Based on the [Alignment Handbook]() we know that we need to use a ~10-100x smaller learning rate for DPO compared to SFT. In our example we reduce the learning rate from 2e-4 (SFT) to 5e-5 (DPO) or 40x smaller.\n",
    "\n",
    "Another important parameter is the `beta` parameter, which is used to control the strength of the alignment. The bigger the beta is typically something in the range of 0.1 to 0.5. A higher beta means less divergence from the initial reference model or the text generations are very similar in terms of their probability distributions. In terms of training length, we go with 1 epoch, which is a good starting point. There is no rule of thumb for the number of epochs, it is also related to the number of epochs used for fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "import wandb\n",
    "\n",
    "args = TrainingArguments(\n",
    "    output_dir=\"zephyr-7b-dpo\",               # directory to save and repository id\n",
    "    num_train_epochs=1,                     # number of training epochs\n",
    "    per_device_train_batch_size=2,         # batch size per device during training\n",
    "    per_device_eval_batch_size=2,           # batch size for evaluation\n",
    "    gradient_accumulation_steps=4,          # number of steps before performing a backward/update pass\n",
    "    gradient_checkpointing=True,            # use gradient checkpointing to save memory\n",
    "    optim=\"adamw_torch_fused\",              # use fused adamw optimizer\n",
    "    learning_rate=5e-5,                     # 10x higher LR than QLoRA paper\n",
    "    max_grad_norm=0.3,                      # max gradient norm based on QLoRA paper\n",
    "    warmup_ratio=0.1,                       # warmup ratio based on QLoRA paper\n",
    "    lr_scheduler_type=\"cosine\",             # use cosine learning rate scheduler\n",
    "    logging_steps=25,                       # log every 25 steps\n",
    "    save_steps=100,                         # when to save checkpoint\n",
    "    save_total_limit=2,                     # limit the total amount of checkpoints\n",
    "    evaluation_strategy=\"steps\",            # evaluate every 1000 steps\n",
    "    eval_steps=700,                         # when to evaluate\n",
    "    bf16=True,                              # use bfloat16 precision\n",
    "    tf32=True,                              # use tf32 precision\n",
    "    push_to_hub=False,                      # push model to hub\n",
    "    #report_to=\"tensorboard\",                # report metrics to tensorboard\n",
    ")\n",
    "\n",
    "dpo_args = {\n",
    "    \"beta\": 0.1,                            # The beta factor in DPO loss. Higher beta means less divergence\n",
    "    \"loss_type\": \"sigmoid\"                  # The loss type for DPO.\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have every building block we need to create our DPOTrainer to start then training our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lole4878/miniconda3/envs/torch212/lib/python3.10/site-packages/trl/trainer/dpo_trainer.py:328: UserWarning: When using DPODataCollatorWithPadding, you should set `remove_unused_columns=False` in your TrainingArguments we have set it for you, but you should do it yourself in the future.\n",
      "  warnings.warn(\n",
      "Map:   0%|          | 0/11000 [00:00<?, ? examples/s]Token indices sequence length is longer than the specified maximum sequence length for this model (2254 > 2048). Running this sequence through the model will result in indexing errors\n",
      "Map: 100%|██████████| 11000/11000 [00:30<00:00, 365.42 examples/s]\n",
      "Map: 100%|██████████| 2750/2750 [00:07<00:00, 343.97 examples/s]\n",
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    }
   ],
   "source": [
    "from trl import DPOTrainer\n",
    "\n",
    "trainer = DPOTrainer(\n",
    "    model,\n",
    "    ref_model=None, # set to none since we use peft\n",
    "    peft_config=peft_config,\n",
    "    args=args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    max_length=max_seq_length,\n",
    "    max_prompt_length=prompt_length,\n",
    "    beta=dpo_args[\"beta\"],\n",
    "    loss_type=dpo_args[\"loss_type\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start training our model by calling the train() method on our Trainer instance. This will start the training loop and train our model for 2 epochs. Since we are using a PEFT method, we will only save the adapted model weights and not the full model.\n",
    "\n",
    "Note: During the training we want to minimize loss and grow reward/margins metrics. Keep an eye on the reward/margins metrics, if they are not growing you might need to increase the beta parameter or adjust the learning_rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start training, the model will be automatically saved to the hub and the output directory\n",
    "trainer.train()\n",
    "\n",
    "# save model at the end of training\n",
    "trainer.save_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training with Flash Attention for 1 epochs with a dataset of ~10k samples took ~01:30:00 on 1x H100 GPU. You should be able to run the training on a g5.2xlarge instance by reducing the batch_size (est. to 1) and maybe the max_seq_length (est. to 1512)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# free the memory again\n",
    "del model\n",
    "del trainer\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optional: Merge LoRA adapter in to the original model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When using QLoRA, we only train adapters and not the full model. This means when saving the model during training we only save the adapter weights and not the full model. If you want to save the full model, which makes it easier to use with Text Generation Inference you can merge the adapter weights into the model weights using the merge_and_unload method and then save the model with the save_pretrained method. This will save a default model, which can be used for inference.\n",
    "\n",
    "Note: You might require > 30GB CPU Memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### COMMENT IN TO MERGE PEFT AND BASE MODEL ####\n",
    "# from peft import PeftModel, PeftConfig\n",
    "# from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "# from peft import AutoPeftModelForCausalLM\n",
    "\n",
    "# # Load PEFT model on CPU\n",
    "# model = AutoPeftModelForCausalLM.from_pretrained(\n",
    "#     args.output_dir,\n",
    "#     torch_dtype=torch.float16,\n",
    "#     low_cpu_mem_usage=True,\n",
    "# )\n",
    "# # Merge LoRA and base model and save\n",
    "# merged_model = model.merge_and_unload()\n",
    "# merged_model.save_pretrained(args.output_dir,safe_serialization=True, max_shard_size=\"2GB\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Test LLM (vibe-check)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the training is done we want to test and evaluate or model. Evaluating Generative AI models in an open-ended way is not a trivial since 1 input can have multiple correct outputs. If you want to learn more about evaluating generative models, check out [Evaluate LLMs and RAG a practical example using Langchain and Hugging Face](https://www.philschmid.de/evaluate-llm) blog post. Especially, when using RLHF techniques like DPO, it's important to \"vibe-check\" the model.\n",
    "\n",
    "This means we want to manually check if the responses are more aligned with what our users or customers want. This could mean that we need to check if the responses are more helpful, more accurate, more engaging, or more informative as before. A good test here is if you have data from your SFT or previous LLMs, you can compare the outputs and see if the new model is better.\n",
    "\n",
    "In our case we just check a few examples and see if the model generates helpful responses using unseen prompts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from peft import AutoPeftModelForCausalLM\n",
    "from transformers import AutoTokenizer, pipeline\n",
    "\n",
    "# Path to saved peft adapter model\n",
    "# peft_model_id = args.output_dir # or\n",
    "peft_model_id = \"./doplhin-dpo\"\n",
    "\n",
    "# Load Model with PEFT adapter\n",
    "model = AutoPeftModelForCausalLM.from_pretrained(\n",
    "  peft_model_id,\n",
    "  device_map=\"auto\",\n",
    "  torch_dtype=torch.float16\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(peft_model_id)\n",
    "# load into pipeline\n",
    "pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We randomely select prom|pts from the teknium/OpenHermes-2.5 dataset and a Hugging Face special."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = [\n",
    "  \"A rectangular garden has a length of 25 feet and a width of 15 feet. If you want to build a fence around the entire garden, how many feet of fencing will you need?\",\n",
    "  \"It's Bengay for muscle relief, a combination of methyl salicylate, menthol, and what other active ingredient commonly found in aspirin?\",\n",
    "  \"How can i get rid of llamas in my backyard?\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets iterate over the prompts and generate a response using the generate method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**Prompt**:\n",
      "A rectangular garden has a length of 25 feet and a width of 15 feet. If you want to build a fence around the entire garden, how many feet of fencing will you need?\n",
      "\n",
      "**Generated Answer**:\n",
      "#### Solution\n",
      "\n",
      "First, we need to find the perimeter of the rectangular garden. The formula for the perimeter of a rectangle is:\n",
      "\n",
      "Perimeter = 2 × (Length + Width)\n",
      "\n",
      "Plugging in the given dimensions:\n",
      "\n",
      "Perimeter = 2 × (25 feet + 15 feet)\n",
      "\n",
      "Perimeter = 2 × (40 feet)\n",
      "\n",
      "Perimeter = 80 feet\n",
      "\n",
      "So, you will need 80 feet of fencing to build a fence around the entire garden.\n",
      "\n",
      "### Example 2\n",
      "\n",
      "A circular garden has a radius of 10 feet. If you want to build a fence around the entire garden, how many feet of fencing will you need?\n",
      "\n",
      "#### Solution\n",
      "\n",
      "To find the perimeter of a circle, we use the formula:\n",
      "\n",
      "Perimeter = 2 × π × Radius\n",
      "\n",
      "Plugging in the given radius:\n",
      "\n",
      "Perimeter = 2 × π × 10 feet\n",
      "\n",
      "Perimeter ≈ 62.83 feet\n",
      "\n",
      "So, you will need approximately 62.83 feet of fencing to build a fence around the entire circular garden.\n",
      "\n",
      "### Example 3\n",
      "\n",
      "A triangular garden has one side of length 20 feet and the other two sides of equal length, 10 feet each. If you want to build a fence around the entire garden, how many feet of fencing will you need?\n",
      "\n",
      "#### Solution\n",
      "\n",
      "In this case, the triangular garden forms a right-angled isosceles triangle with two equal sides of 10 feet each and the third side of 20 feet. To find the perimeter of the triangle, we add up the lengths of all three sides.\n",
      "\n",
      "Perimeter = Side1 + Side2 + Side3\n",
      "\n",
      "Perimeter = 20 feet + 10 feet + 10 feet\n",
      "\n",
      "Perimeter = 40 feet\n",
      "\n",
      "So, you will need 40 feet of fencing to build a fence around the entire triangular garden.\n",
      "\n",
      "## Conclusion\n",
      "\n",
      "To determine the amount of fencing needed to enclose a garden, calculate the perimeter of the garden's shape using the appropriate formula. In general, if the garden is rectangular, circular, or triangular, you can use the formulas provided in the examples above. Remember that the cost of fencing materials and installation may vary depending on the type and quality of fencing you choose, so be sure to factor in these expenses as well when planning your project.\n",
      "\n",
      "## You might also be interested in\n",
      "\n",
      "- How to calculate the perimeter of a square garden.\n",
      "- How to calculate the perimeter of different shapes like triangles, rectangles, circles, and more.\n",
      "- How to determine the cost of materials for fencing projects.\n",
      "- Factors to consider when choosing the right type of fencing for your garden.\n",
      "- Steps to install a fence around your garden.\n",
      "- Maintenance tips for keeping your garden fence in good condition.\n",
      "- DIY versus hiring a professional to install your garden fence.лизм в саду: как правильно выбрать и установить заборлизм в саду: как правильно выбрать и установить заборлизм в саду: как правильно выбрать и установить заборлизм в саду: как правильно выбрать и установить заборлизм в саду: как правильно выбрать и установить заборлизм в саду: как правильно выбрать и установить забор индексы: исполнительная служба, забор, дизайн, сад, дизайн забора, цветной забор, забор для сада, забор интересный, интересный забор, интересный забор для сада, забор с элементами искусства, дизайн для сада, дизайн забора, интересный дизайн забора, забор для сада, забор с элементами искусства, дизайн сада, забор интересный, интересный забор, интересный забор для сада, забор с элементами искусства, дизайн для сада, дизайн забора, интересный дизайн забора, забор для сада, забор с элементами искусства, дизайн сада, забор интересный, интересный забор, интересный забор для сада, забор с элементами искусства, дизайн для сада, дизайн забора, интересный дизайн забора, забор для сада, забор с элементами искусства, дизайн сада, забор интересный, интересный забор, интересный забор для сада, забор с элементами искусства, дизайн для сада, дизайн забора, интересный дизайн забора, забор для сада, забор с элементами искусства, дизайн сада, забор интересный, интересный забор, интересный забор для сада, забор с элементами искусства, дизайн для сада, дизайн забора, интересный дизайн забора, забор для сада, забор с элементами искусства, дизайн сада, забор интересный, интересный забор, интересный забор для сада, забор с элементами искусства, дизайн для сада, дизайн забора, интересный дизайн забора, забор для сада, забор с элементами искусства, дизайн сада, забор интересный, интересный забор, интересный забор для сада, забор с элементами искусства, дизайн для сада, дизайн забора, интересный дизайн забора, забор для сада, забор с элементами искусства, дизайн сада, забор интересный, интересный забор, интересный забор для сада, забор с элементами искусства, дизайн для сада, дизайн забора, интересный дизайн забора, забор для сада, забор с элементами искусства, дизайн сада, забор интересный, интересный забор, интересный забор для сада, забор с элементами искусства, дизайн для сада, дизайн забора, интересный дизайн забора, забор для сада, забор с элементами искусства, дизайн сада, забор интересный, интересный забор, интересный забор для сада, забор с элементами искусства, дизайн для сада, дизайн забора, интересный дизайн забора, забор для сада, забор с элементами искусства, дизайн сада, забор интересный, интересный забор, интересный забор для сада, забор с элементами искусства, дизайн для сада, дизайн забора, интересный дизайн забора, забор для сада, забор с элементами искусства, дизайн сада, забор интересный, интересный забор, интересный забор для сада, забор с элементами искусства, дизайн для сада, дизайн забора, интересный дизайн забора, забор для сада, забор с элементами искусства, дизайн сада, забор интересный, интересный забор, интересный забор для сада, забор с элементами искусства, дизайн для сада, дизайн забора, интересный дизайн забора, забор для сада, забор с элементами искусства, дизайн сада, забор интересный, интересный забор, интересный забор для сада, забор с элементами искусства, дизайн для сада, дизайн забора, интересный дизайн забора, забор для сада, забор с элементами искусства,\n",
      "==============================\n",
      "**Prompt**:\n",
      "It's Bengay for muscle relief, a combination of methyl salicylate, menthol, and what other active ingredient commonly found in aspirin?\n",
      "\n",
      "**Generated Answer**:\n",
      "Answer:\n",
      "\n",
      "Camphor\n",
      "\n",
      "Explanation:\n",
      "\n",
      "Bengay is a topical analgesic and muscle pain relief ointment that contains the active ingredients methyl salicylate, menthol, and camphor. The combination of these ingredients works to soothe sore muscles and joints, providing relief from minor aches and pains.\n",
      "\n",
      "Methyl salicylate is a derivative of salicylic acid, which is found in aspirin. It provides a warming sensation when applied to the skin and helps to relieve pain and inflammation.\n",
      "\n",
      "Menthol is a cooling agent that creates a cold sensation on the skin. It works to stimulate cold receptors in the skin, which can help to relieve pain by distracting the brain from the source of the pain.\n",
      "\n",
      "Camphor is an organic compound with a strong, penetrating odor. It has been used for centuries for its medicinal properties, including its ability to relieve pain and reduce inflammation. Camphor also has a warming effect when applied to the skin, helping to soothe aching muscles and joints.\n",
      "\n",
      "In addition to these active ingredients, Bengay also contains other inactive ingredients, such as petroleum jelly, which helps the other ingredients to spread evenly across the skin and provides a protective barrier to lock in the medicine.\n",
      "\n",
      "Bengay comes in various forms, including ointments, gels, and roll-ons, making it easy to apply to the affected area and get relief from minor muscle aches and pains. However, it is always important to follow the directions on the packaging and not to apply Bengay to broken or irritated skin, as it may cause further irritation or discomfort.\n",
      "==============================\n",
      "**Prompt**:\n",
      "How can i get rid of llamas in my backyard?\n",
      "\n",
      "**Generated Answer**:\n",
      "The Humane Society suggests contacting a llama rescue organization, such as Llama Rescue Inc. or Llama Haven. Rehoming a llama can be a long process, so you'll need to be patient. You can also try contacting local farmers or ranchers to see if anyone wants a llama.\n",
      "\n",
      "## Can you have a pet llama in your backyard?\n",
      "\n",
      "In many parts of the country, you can indeed have a pet llama in your backyard. However, in other parts, this is either highly regulated or strictly prohibited. It's critical that you check local zoning and ordinance laws, as well as homeowners association rules if you have one.\n",
      "\n",
      "## How do you take care of a pet llama?\n",
      "\n",
      "Llamas need access to fresh water at all times and should receive regular hay and pellet feedings, supplemented by fresh fruits and vegetables. They also need to graze on grass, which helps keep their teeth healthy. Llamas should be groomed at least once a week to remove dirt and debris from their fleece.\n",
      "\n",
      "## Are llamas hard to care for?\n",
      "\n",
      "Llamas are fairly easy to care for, especially in comparison to other large animals. They are not as large as horses, which require much more space and care. Llamas are very low maintenance when it comes to feeding. They eat hay and pellets and occasionally enjoy vegetables.\n",
      "\n",
      "## Can I keep a llama as a pet?\n",
      "\n",
      "Yes, you can keep a llama as a pet. They can live in a variety of climates and living situations, but it's very important that you do your research before deciding to become a llama owner. They require space, socialization, proper diet and routine veterinary care, just like any other pet.\n",
      "\n",
      "## How much does it cost to keep a pet llama?\n",
      "\n",
      "The average cost to keep a llama can range from $1,000 to $4,000 a year, but it can vary depending on your location, the size of your property, and any special needs your llama might have.\n",
      "\n",
      "## How long can a llama live as a pet?\n",
      "\n",
      "Llamas can live for 15-20 years as pets, sometimes even longer. They may live longer if they're well cared for and don't develop any health issues.\n",
      "\n",
      "## What is a pet llama called?\n",
      "\n",
      "Just like dogs, cats, and other domesticated animals, a pet llama is simply called a pet llama. However, you might hear them referred to as companion llamas, which is another term for a llama that is kept for companionship, instead of for work or breeding purposes.\n",
      "\n",
      "## How many times a day do pet llamas need to be fed?\n",
      "\n",
      "As a general rule, pet llamas should be fed twice a day. However, the exact amount and frequency of feedings can vary depending on the llama's age, size, activity level, and overall health.\n",
      "\n",
      "## Can you have a llama as a pet in California?\n",
      "\n",
      "Yes, you can have a llama as a pet in California. However, it's very important that you check local zoning and ordinance laws, as well as homeowners association rules if you have one. Llamas are legal to own and keep in most areas of the state, but there may be some restrictions on where you can keep them.\n",
      "\n",
      "## What do I need to know about keeping a llama as a pet?\n",
      "\n",
      "Before deciding to get a llama as a pet, there are a few important things to consider. Llamas are social animals and need regular interaction with humans and other llamas. They require a proper diet, including hay, pellets, and occasional vegetables. They also need routine veterinary care, such as vaccinations, deworming, and hoof care.\n",
      "\n",
      "## Are llamas allowed as pets in the US?\n",
      "\n",
      "Yes, llamas are allowed as pets in the United States. However, it's very important that you check local zoning and ordinance laws, as well as homeowners association rules if you have one. Llamas are legal to own and keep in most areas of the country, but there may be some restrictions on where you can keep them.\n",
      "\n",
      "## What do I need to know about caring for a pet llama?\n",
      "\n",
      "Caring for a pet llama is similar to caring for other pets, but there are some unique considerations. Llamas are social animals and need regular interaction with humans and other llamas. They require a proper diet, including hay, pellets, and occasional vegetables. They also need routine veterinary care, such as vaccinations, deworming, and hoof care.\n",
      "\n",
      "## Can you have a pet llama in an apartment?\n",
      "\n",
      "It's not recommended to have a pet llama in an apartment. Llamas need plenty of space to roam and graze, and they may not be allowed in apartment complexes due to zoning laws and homeowners association rules.\n",
      "\n",
      "## Can you have a pet llama in a city?\n",
      "\n",
      "It's possible to have a pet llama in a city, but it depends on your specific location and local laws. It's very important that you check local zoning and ordinance laws, as well as homeowners association rules if you have one. Llamas are legal to own and keep in most areas of the country, but there may be some restrictions on where you can keep them.\n",
      "\n",
      "## Can you have a pet llama in a small yard?\n",
      "\n",
      "It's possible to have a pet llama in a small yard, but it will require some planning and modifications to your yard. Llamas need plenty of space to roam and graze, so you'll need to provide adequate fencing and space for your llama. You should also consider adding shelter from the elements and other amenities to make sure your llama stays healthy and comfortable.\n",
      "\n",
      "## Can you have a pet llama in the UK?\n",
      "\n",
      "Yes, you can have a pet llama in the UK. However, it's very important that you check local zoning and ordinance laws, as well as homeowners association rules if you have one. Llamas are legal to own and keep in most areas of the country, but there may be some restrictions on where you can keep them.\n",
      "\n",
      "## Can you have a pet llama in the UK without a license?\n",
      "\n",
      "In most cases, you do not need a license to keep a pet llama in the UK. However, there may be some local restrictions or requirements, so it's important to check with your local council or authority to make sure you're in compliance with all laws and regulations.\n",
      "\n",
      "## Do pet llamas need a lot of space?\n",
      "\n",
      "Yes, pet llamas do need a lot of space. They need enough room to roam and graze, and they'll also need a proper shelter to protect them from the elements. Depending on the size of your property and the number of llamas you have, you may need to invest in fencing and other amenities to ensure your llamas have a safe and comfortable environment.\n",
      "\n",
      "## Can you have a pet llama in Florida?\n",
      "\n",
      "Yes, you can have a pet llama in Florida. However, it's very important that you check local zoning and ordinance laws, as well as homeowners association rules if you have one. Llamas are legal to own and keep in most areas of the state, but there may be some restrictions on where you can keep them.\n",
      "\n",
      "## Can you have a pet llama in New York?\n",
      "\n",
      "Yes, you can have a pet llama in New York. However, it's very important that you check local zoning and ordinance laws, as well as homeowners association rules if you have one. Llamas are legal to own and keep in most areas of the state, but there may be some restrictions on where you can keep them.\n",
      "\n",
      "## Can you have a pet llama in Texas?\n",
      "\n",
      "Yes, you can have a pet llama in Texas. However, it's very important that you check local zoning and ordinance laws, as well as homeowners association rules if you have one. Llamas are legal to own and keep in most areas of the state, but there may be some restrictions on where you can keep them.\n",
      "\n",
      "## Can you have a pet llama in Australia?\n",
      "\n",
      "Yes, you can have a pet llama in Australia. However, it's very important that you check local zoning and ordinance laws, as well as homeowners association rules if you have one. Llamas are legal to own and keep in most areas of the country, but there may be some restrictions on where you can keep them.\n",
      "\n",
      "## Can you have a pet llama in Canada?\n",
      "\n",
      "Yes, you can have a pet llama in Canada. However, it's very important that you check local zoning and ordinance laws, as well as homeowners association rules if you have one. Llamas are legal to own and keep in most areas of the country, but there may be some restrictions on where you can keep them.\n",
      "\n",
      "## How many acres do I need to keep a pet llama?\n",
      "\n",
      "The amount of acreage you'll need to keep a pet llama depends on several factors, including the number of llamas you have and their individual needs. As a general rule, you'll need at least one acre of fenced-\n",
      "==============================\n"
     ]
    }
   ],
   "source": [
    "for prompt in prompts:\n",
    "  messages = pipe.tokenizer.apply_chat_template([{\"role\":\"user\", \"content\": prompt}], tokenize=False)\n",
    "  outputs = pipe(prompt, max_new_tokens=2048, do_sample=True, temperature=1.0, top_k=50, top_p=0.9, eos_token_id=tokenizer.eos_token_id, pad_token_id=tokenizer.pad_token_id)\n",
    "  print(f\"**Prompt**:\\n{prompt}\\n\")\n",
    "  print(f\"**Generated Answer**:\\n{outputs[0]['generated_text'][len(prompt):].strip()}\")\n",
    "  print(\"===\" * 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Evaluate open LLMs on MT-Bench"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our use case we will use [MT-Bench](https://github.com/lm-sys/FastChat/blob/main/fastchat/llm_judge/README.md), a Benchmark designed by LMSYS to test the conversation and instruction-following capabilities of LLMs. It evaluates LLMs through multi-turn conversations, focusing on their ability to engage in coherent, informative, and engaging exchanges. Since human evaluation is very expensive and time consuming, LMSYS uses GPT-4-Turbo to grade the model responses. Their paper shows as 80% agreement between strong LLM and human preferences. The LMSYS leaderboard is updated regularly (last updated February 2, 2024). MT-Bench is part of the [FastChat Repository](https://github.com/lm-sys/FastChat/blob/main/fastchat/llm_judge/README.md).\n",
    "\n",
    "MT-Bench supports two different evaluation stratgies:\n",
    "\n",
    "- single-answer grading: LLM grade and give a score to model's answer directly on a scale of 10\n",
    "- pair-wise comparison: Compare two models and see which one is better using LLM as judge, resulting in a win-rate.\n",
    "\n",
    "We are going to use the pair-wise comparison method to compare the base SFT Model with the DPO model, to see if aligning the model with DPO improved the model. Running pairwise comparison on MT-Bench includes the following steps:\n",
    "\n",
    "- Clone the FastChat Repository & install the requirements\n",
    "- Generate Responses using our SFT (original) & DPO (trained) model\n",
    "- Evaluate the responses using pair-wise comparison and GPT-4-Turbo as Judge\n",
    "- Plot and compare the results\n",
    "\n",
    "MT-Bench currenlty only support OpenAI or Anthropic as Judge, where GPT-4 is the best. If you don't have access to GPT-4 you need to use a different evaluation method. I forked the FastChat repository and added GPT-4 Turbo reference answers to keep the cost lower.\n",
    "\n",
    "Note: If you use this example to train different model, e.g. llama you need to make sure that your model is registered and support in FastChat. This means you need:\n",
    "\n",
    "- [a registered conversation template](https://github.com/lm-sys/FastChat/blob/1db84d0906196673db361eac50d5aa65180a0ffe/fastchat/conversation.py#L1024-L1035)\n",
    "- [a moodel adapter](https://github.com/lm-sys/FastChat/blob/1db84d0906196673db361eac50d5aa65180a0ffe/fastchat/model/model_adapter.py#L1491-L1504) used to match the model path\n",
    "- [register the model adapter](https://github.com/lm-sys/FastChat/blob/1db84d0906196673db361eac50d5aa65180a0ffe/fastchat/model/model_adapter.py#L2255)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Clone the FastChat Repository & install the requirements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by cloning the FastChat repository and installing the requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone main branch of FastChat\n",
    "!git clone https://github.com/philschmid/FastChat.git\n",
    "# Install FastChat with model worker and llm_judge dependencies\n",
    "!pip install -e \"./FastChat[model_worker,llm_judge]\"\n",
    "!pip install matplotlib tabulate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate Responses using our SFT (original) & DPO (trained) model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To Generate the responses in MT-Bench we need our directory into FastChat/fastchat/llm_judge and then run the gen_model_answer.py script. This will generate the responses and save them into a file. We will use the default --max-new-token length of 1024, which could lead to some truncation. If you want to avoid truncation you can increase the --max-new-token length to 1512 or higher.\n",
    "\n",
    "We change into the FastChat/fastchat/llm_judge directory to run all the evaluation scripts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/share/home/lole4878/workspace/dpo/test/FastChat/fastchat/llm_judge\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "%cd {os.getcwd()}/FastChat/fastchat/llm_judge\n",
    "# should be in FastChat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets start with the SFT model and then the DPO model.\n",
    "\n",
    "Note: The answer of the models will be stored to FastChat/fastchat/llm_judge/data/mt_bench/model_answer. You might want to save them later for additional evaluation, when you have a new fine-tuned model. Generating all responses can take ~60 minutes or more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/bash: /home/lole4878/miniconda3/lib/libtinfo.so.6: no version information available (required by /bin/bash)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output to data/mt_bench/model_answer/mistral-dolphin-sft.jsonl\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Loading checkpoint shards:   0%|                          | 0/2 [00:00<?, ?it/s]/home/lole4878/miniconda3/envs/torch212/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "Loading checkpoint shards: 100%|██████████████████| 2/2 [00:01<00:00,  1.99it/s]\n",
      "  0%|                                                    | 0/80 [00:00<?, ?it/s]/home/lole4878/miniconda3/envs/torch212/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:392: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "  1%|▌                                         | 1/80 [00:50<1:06:25, 50.44s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "  2%|█                                         | 2/80 [01:40<1:05:09, 50.12s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "  4%|█▋                                          | 3/80 [02:20<58:17, 45.42s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "  5%|██                                        | 4/80 [03:10<1:00:00, 47.38s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "  6%|██▋                                       | 5/80 [04:03<1:01:32, 49.24s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "  8%|███▏                                      | 6/80 [04:52<1:00:56, 49.42s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "  9%|███▊                                        | 7/80 [05:19<51:12, 42.09s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 10%|████▍                                       | 8/80 [06:11<54:04, 45.06s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 11%|████▉                                       | 9/80 [07:02<55:44, 47.10s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 12%|█████▍                                     | 10/80 [07:30<47:47, 40.97s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 14%|█████▉                                     | 11/80 [08:20<50:29, 43.90s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 15%|██████▍                                    | 12/80 [08:47<43:59, 38.82s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 16%|██████▉                                    | 13/80 [09:39<47:39, 42.68s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 18%|███████▌                                   | 14/80 [10:30<49:49, 45.30s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 19%|████████                                   | 15/80 [11:21<50:52, 46.96s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 20%|████████▌                                  | 16/80 [12:13<51:42, 48.47s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 21%|█████████▏                                 | 17/80 [12:51<47:29, 45.24s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 22%|█████████▋                                 | 18/80 [13:18<41:06, 39.78s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 24%|██████████▏                                | 19/80 [13:45<36:37, 36.03s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 25%|██████████▊                                | 20/80 [14:36<40:21, 40.36s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 26%|███████████▎                               | 21/80 [15:26<42:41, 43.41s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 28%|███████████▊                               | 22/80 [15:54<37:29, 38.78s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 29%|████████████▎                              | 23/80 [16:21<33:27, 35.22s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 30%|████████████▉                              | 24/80 [17:12<37:13, 39.88s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 31%|█████████████▍                             | 25/80 [17:41<33:42, 36.77s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 32%|█████████████▉                             | 26/80 [18:31<36:38, 40.71s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 34%|██████████████▌                            | 27/80 [19:22<38:38, 43.75s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 35%|███████████████                            | 28/80 [20:14<40:03, 46.23s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 36%|███████████████▌                           | 29/80 [21:05<40:35, 47.76s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 38%|████████████████▏                          | 30/80 [21:56<40:38, 48.77s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 39%|████████████████▋                          | 31/80 [22:48<40:22, 49.45s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 40%|█████████████████▏                         | 32/80 [23:38<39:46, 49.71s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 41%|█████████████████▋                         | 33/80 [24:29<39:10, 50.01s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 42%|██████████████████▎                        | 34/80 [24:57<33:24, 43.57s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 44%|██████████████████▊                        | 35/80 [25:48<34:24, 45.88s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 45%|███████████████████▎                       | 36/80 [26:17<29:51, 40.71s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 46%|███████████████████▉                       | 37/80 [27:07<31:11, 43.52s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 48%|████████████████████▍                      | 38/80 [27:44<29:06, 41.58s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 49%|████████████████████▉                      | 39/80 [28:35<30:14, 44.26s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 50%|█████████████████████▌                     | 40/80 [29:25<30:43, 46.09s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 51%|██████████████████████                     | 41/80 [30:17<31:04, 47.82s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 52%|██████████████████████▌                    | 42/80 [31:09<31:01, 48.99s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 54%|███████████████████████                    | 43/80 [31:46<28:02, 45.48s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 55%|███████████████████████▋                   | 44/80 [32:37<28:15, 47.10s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 56%|████████████████████████▏                  | 45/80 [33:30<28:33, 48.95s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 57%|████████████████████████▋                  | 46/80 [34:08<25:57, 45.80s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 59%|█████████████████████████▎                 | 47/80 [35:01<26:14, 47.70s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 60%|█████████████████████████▊                 | 48/80 [35:53<26:06, 48.97s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 61%|██████████████████████████▎                | 49/80 [36:43<25:30, 49.36s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 62%|██████████████████████████▉                | 50/80 [37:35<25:08, 50.28s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 64%|███████████████████████████▍               | 51/80 [38:28<24:40, 51.04s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 65%|███████████████████████████▉               | 52/80 [39:19<23:49, 51.04s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 66%|████████████████████████████▍              | 53/80 [40:10<22:56, 50.97s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 68%|█████████████████████████████              | 54/80 [41:02<22:11, 51.20s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 69%|█████████████████████████████▌             | 55/80 [41:52<21:10, 50.84s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 70%|██████████████████████████████             | 56/80 [42:42<20:13, 50.57s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 71%|██████████████████████████████▋            | 57/80 [43:33<19:31, 50.92s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 72%|███████████████████████████████▏           | 58/80 [44:26<18:53, 51.50s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 74%|███████████████████████████████▋           | 59/80 [45:16<17:53, 51.11s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 75%|████████████████████████████████▎          | 60/80 [45:45<14:48, 44.43s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 76%|████████████████████████████████▊          | 61/80 [46:36<14:42, 46.43s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 78%|█████████████████████████████████▎         | 62/80 [47:28<14:25, 48.08s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 79%|█████████████████████████████████▊         | 63/80 [48:18<13:45, 48.57s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 80%|██████████████████████████████████▍        | 64/80 [49:09<13:08, 49.29s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 81%|██████████████████████████████████▉        | 65/80 [49:59<12:21, 49.47s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 82%|███████████████████████████████████▍       | 66/80 [50:36<10:39, 45.71s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 84%|████████████████████████████████████       | 67/80 [51:25<10:09, 46.89s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 85%|████████████████████████████████████▌      | 68/80 [52:02<08:46, 43.84s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 86%|█████████████████████████████████████      | 69/80 [52:52<08:22, 45.66s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 88%|█████████████████████████████████████▋     | 70/80 [53:44<07:56, 47.64s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 89%|██████████████████████████████████████▏    | 71/80 [54:36<07:19, 48.83s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 90%|██████████████████████████████████████▋    | 72/80 [55:26<06:34, 49.25s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 91%|███████████████████████████████████████▏   | 73/80 [56:17<05:47, 49.65s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 92%|███████████████████████████████████████▊   | 74/80 [56:54<04:36, 46.08s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 94%|████████████████████████████████████████▎  | 75/80 [57:45<03:57, 47.49s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 95%|████████████████████████████████████████▊  | 76/80 [58:37<03:15, 48.81s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 96%|█████████████████████████████████████████▍ | 77/80 [59:28<02:28, 49.40s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 98%|███████████████████████████████████████▉ | 78/80 [1:00:07<01:32, 46.26s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 99%|████████████████████████████████████████▍| 79/80 [1:00:59<00:47, 47.97s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "100%|█████████████████████████████████████████| 80/80 [1:01:48<00:00, 46.36s/it]\n"
     ]
    }
   ],
   "source": [
    "# make sure that is the correct path\n",
    "model_path=\"cognitivecomputations/dolphin-2.1-mistral-7b\"\n",
    "# model id will be used to load our conversation template https://github.com/lm-sys/FastChat/blob/1db84d0906196673db361eac50d5aa65180a0ffe/fastchat/model/model_adapter.py#L1579\n",
    "model_id=\"mistral-dolphin-sft\"\n",
    "\n",
    "# generate model answer\n",
    "!python gen_model_answer.py --model-id {model_id} --model-path {model_path}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we generate the responses using the DPO model. Generating all responses can take ~120 minutes or more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/bash: /home/lole4878/miniconda3/lib/libtinfo.so.6: no version information available (required by /bin/bash)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output to data/mt_bench/model_answer/mistral-dolphin-dpo.jsonl\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Loading checkpoint shards:   0%|                          | 0/2 [00:00<?, ?it/s]/home/lole4878/miniconda3/envs/torch212/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "Loading checkpoint shards: 100%|██████████████████| 2/2 [00:01<00:00,  1.95it/s]\n",
      "  0%|                                                    | 0/80 [00:00<?, ?it/s]/home/lole4878/miniconda3/envs/torch212/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:392: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "  1%|▌                                           | 1/80 [00:16<21:53, 16.62s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "  2%|█                                           | 2/80 [00:59<41:32, 31.96s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "  4%|█▋                                          | 3/80 [01:32<41:51, 32.61s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "  5%|██▏                                         | 4/80 [02:29<53:24, 42.17s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "  6%|██▊                                         | 5/80 [02:42<39:44, 31.80s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "  8%|███▎                                        | 6/80 [03:39<49:29, 40.12s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "  9%|███▊                                        | 7/80 [04:17<47:53, 39.37s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 10%|████▍                                       | 8/80 [04:46<43:34, 36.31s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 11%|████▉                                       | 9/80 [05:08<37:27, 31.65s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 12%|█████▍                                     | 10/80 [06:02<44:57, 38.54s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 14%|█████▉                                     | 11/80 [06:56<50:02, 43.51s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 15%|██████▍                                    | 12/80 [07:40<49:17, 43.49s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 16%|██████▉                                    | 13/80 [08:10<43:57, 39.37s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 18%|███████▌                                   | 14/80 [09:01<47:10, 42.89s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 19%|████████                                   | 15/80 [09:16<37:16, 34.41s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 20%|████████▌                                  | 16/80 [09:38<32:54, 30.85s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 21%|█████████▏                                 | 17/80 [09:51<26:47, 25.52s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 22%|█████████▋                                 | 18/80 [10:14<25:23, 24.58s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 24%|██████████▏                                | 19/80 [11:10<34:37, 34.05s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 25%|██████████▊                                | 20/80 [11:44<34:00, 34.01s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 26%|███████████▎                               | 21/80 [12:12<31:38, 32.18s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 28%|███████████▊                               | 22/80 [12:49<32:40, 33.80s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 29%|████████████▎                              | 23/80 [13:45<38:27, 40.48s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 30%|████████████▉                              | 24/80 [14:17<35:24, 37.94s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 31%|█████████████▍                             | 25/80 [14:19<24:56, 27.22s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 32%|█████████████▉                             | 26/80 [14:54<26:27, 29.40s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 34%|██████████████▌                            | 27/80 [15:00<19:44, 22.35s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 35%|███████████████                            | 28/80 [15:07<15:19, 17.68s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 36%|███████████████▌                           | 29/80 [15:59<23:52, 28.09s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 38%|████████████████▏                          | 30/80 [16:32<24:42, 29.64s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 39%|████████████████▋                          | 31/80 [17:27<30:15, 37.06s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 40%|█████████████████▏                         | 32/80 [17:51<26:37, 33.28s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 41%|█████████████████▋                         | 33/80 [18:22<25:36, 32.69s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 42%|██████████████████▎                        | 34/80 [19:07<27:44, 36.19s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 44%|██████████████████▊                        | 35/80 [19:42<26:54, 35.88s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 45%|███████████████████▎                       | 36/80 [20:09<24:21, 33.22s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 46%|███████████████████▉                       | 37/80 [20:38<22:57, 32.03s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 48%|████████████████████▍                      | 38/80 [21:11<22:33, 32.23s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 49%|████████████████████▉                      | 39/80 [21:15<16:13, 23.73s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 50%|█████████████████████▌                     | 40/80 [22:09<21:50, 32.77s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 51%|██████████████████████                     | 41/80 [22:34<19:54, 30.63s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 52%|██████████████████████▌                    | 42/80 [23:13<20:54, 33.01s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 54%|███████████████████████                    | 43/80 [24:07<24:11, 39.22s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 55%|███████████████████████▋                   | 44/80 [24:28<20:18, 33.84s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 56%|████████████████████████▏                  | 45/80 [24:47<17:10, 29.46s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 57%|████████████████████████▋                  | 46/80 [25:05<14:45, 26.05s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 59%|█████████████████████████▎                 | 47/80 [25:46<16:43, 30.41s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 60%|█████████████████████████▊                 | 48/80 [26:07<14:49, 27.78s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 61%|██████████████████████████▎                | 49/80 [26:38<14:47, 28.62s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 62%|██████████████████████████▉                | 50/80 [27:18<16:04, 32.16s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 64%|███████████████████████████▍               | 51/80 [27:57<16:27, 34.04s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 65%|███████████████████████████▉               | 52/80 [28:52<18:48, 40.29s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 66%|████████████████████████████▍              | 53/80 [29:23<16:58, 37.72s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 68%|█████████████████████████████              | 54/80 [30:06<16:54, 39.03s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 69%|█████████████████████████████▌             | 55/80 [30:50<16:59, 40.79s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 70%|██████████████████████████████             | 56/80 [31:27<15:47, 39.48s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 71%|██████████████████████████████▋            | 57/80 [32:08<15:17, 39.90s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 72%|███████████████████████████████▏           | 58/80 [32:32<12:55, 35.23s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 74%|███████████████████████████████▋           | 59/80 [33:27<14:22, 41.05s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 75%|████████████████████████████████▎          | 60/80 [33:46<11:28, 34.42s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 76%|████████████████████████████████▊          | 61/80 [34:41<12:51, 40.63s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 78%|█████████████████████████████████▎         | 62/80 [35:10<11:12, 37.34s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 79%|█████████████████████████████████▊         | 63/80 [35:47<10:30, 37.11s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 80%|██████████████████████████████████▍        | 64/80 [36:17<09:21, 35.08s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 81%|██████████████████████████████████▉        | 65/80 [36:51<08:38, 34.60s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 82%|███████████████████████████████████▍       | 66/80 [37:23<07:56, 34.02s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 84%|████████████████████████████████████       | 67/80 [37:48<06:47, 31.32s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 85%|████████████████████████████████████▌      | 68/80 [38:38<07:20, 36.70s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 86%|█████████████████████████████████████      | 69/80 [39:16<06:48, 37.13s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 88%|█████████████████████████████████████▋     | 70/80 [39:47<05:53, 35.37s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 89%|██████████████████████████████████████▏    | 71/80 [39:53<03:59, 26.65s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 90%|██████████████████████████████████████▋    | 72/80 [40:08<03:03, 22.95s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 91%|███████████████████████████████████████▏   | 73/80 [40:57<03:35, 30.78s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 92%|███████████████████████████████████████▊   | 74/80 [41:22<02:54, 29.12s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 94%|████████████████████████████████████████▎  | 75/80 [41:31<01:55, 23.11s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 95%|████████████████████████████████████████▊  | 76/80 [42:11<01:52, 28.00s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 96%|█████████████████████████████████████████▍ | 77/80 [42:16<01:03, 21.21s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 98%|█████████████████████████████████████████▉ | 78/80 [42:58<00:54, 27.43s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 99%|██████████████████████████████████████████▍| 79/80 [43:53<00:35, 35.87s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "100%|███████████████████████████████████████████| 80/80 [44:48<00:00, 33.61s/it]\n"
     ]
    }
   ],
   "source": [
    "# Change this to where you saved the model during training, remember our current directory is FastChat/\n",
    "model_path=\"../../../doplhin-dpo\"\n",
    "\n",
    "# model id will be used to load our conversation template https://github.com/lm-sys/FastChat/blob/1db84d0906196673db361eac50d5aa65180a0ffe/fastchat/model/model_adapter.py#L1579\n",
    "model_id=\"mistral-dolphin-dpo\"\n",
    "\n",
    "# generate model answer\n",
    "!python gen_model_answer.py --model-id {model_id} --model-path {model_path}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate the responses using pair-wise comparison and GPT-4-Turbo as Judge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After we have the responses we can evaluate them using the gen_judgment.py script. This will pairwise compare all the responses using GPT-4-Turbo and rate which response is better.\n",
    "\n",
    "Note: We need an OPENAI_API_KEY with access to GPT-4 Turbo, running MT-Bench will cost ~1-2$ per model evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/bash: /home/lole4878/miniconda3/lib/libtinfo.so.6: no version information available (required by /bin/bash)\n",
      "Stats:\n",
      "{\n",
      "    \"bench_name\": \"mt_bench\",\n",
      "    \"mode\": \"pairwise-all\",\n",
      "    \"judge\": \"gpt-4-1106-preview\",\n",
      "    \"baseline\": null,\n",
      "    \"model_list\": [\n",
      "        \"mistral-dolphin-dpo\",\n",
      "        \"mistral-dolphin-sft\"\n",
      "    ],\n",
      "    \"total_num_questions\": 80,\n",
      "    \"total_num_matches\": 160,\n",
      "    \"output_path\": \"data/mt_bench/model_judgment/gpt-4-1106-preview_pair.jsonl\"\n",
      "}\n",
      "Evaluating the following models.\n",
      "  0%|                                                   | 0/160 [00:00<?, ?it/s]question: 81, turn: 1, model_1: mistral-dolphin-dpo, model_2: mistral-dolphin-sft, g1_winner: tie, g2_winner: tie, judge: ('gpt-4-1106-preview', 'pair-v2')\n",
      "  1%|▎                                        | 1/160 [00:56<2:28:59, 56.22s/it]question: 82, turn: 1, model_1: mistral-dolphin-dpo, model_2: mistral-dolphin-sft, g1_winner: tie, g2_winner: tie, judge: ('gpt-4-1106-preview', 'pair-v2')\n",
      "  1%|▌                                        | 2/160 [01:21<1:39:27, 37.77s/it]question: 83, turn: 1, model_1: mistral-dolphin-dpo, model_2: mistral-dolphin-sft, g1_winner: tie, g2_winner: tie, judge: ('gpt-4-1106-preview', 'pair-v2')\n",
      "  2%|▊                                          | 3/160 [01:22<55:54, 21.37s/it]question: 84, turn: 1, model_1: mistral-dolphin-dpo, model_2: mistral-dolphin-sft, g1_winner: model_1, g2_winner: model_1, judge: ('gpt-4-1106-preview', 'pair-v2')\n",
      "  2%|█                                          | 4/160 [01:47<59:05, 22.73s/it]question: 85, turn: 1, model_1: mistral-dolphin-dpo, model_2: mistral-dolphin-sft, g1_winner: tie, g2_winner: tie, judge: ('gpt-4-1106-preview', 'pair-v2')\n",
      "  3%|█▎                                         | 5/160 [01:59<48:41, 18.85s/it]question: 86, turn: 1, model_1: mistral-dolphin-dpo, model_2: mistral-dolphin-sft, g1_winner: model_1, g2_winner: tie, judge: ('gpt-4-1106-preview', 'pair-v2')\n",
      "  4%|█▌                                       | 6/160 [02:41<1:08:44, 26.78s/it]question: 87, turn: 1, model_1: mistral-dolphin-dpo, model_2: mistral-dolphin-sft, g1_winner: tie, g2_winner: tie, judge: ('gpt-4-1106-preview', 'pair-v2')\n",
      "  4%|█▉                                         | 7/160 [02:43<47:31, 18.64s/it]question: 88, turn: 1, model_1: mistral-dolphin-dpo, model_2: mistral-dolphin-sft, g1_winner: model_1, g2_winner: tie, judge: ('gpt-4-1106-preview', 'pair-v2')\n",
      "  5%|██▏                                        | 8/160 [03:11<54:41, 21.59s/it]question: 89, turn: 1, model_1: mistral-dolphin-dpo, model_2: mistral-dolphin-sft, g1_winner: tie, g2_winner: tie, judge: ('gpt-4-1106-preview', 'pair-v2')\n",
      "  6%|██▍                                        | 9/160 [03:38<58:44, 23.34s/it]question: 90, turn: 1, model_1: mistral-dolphin-dpo, model_2: mistral-dolphin-sft, g1_winner: tie, g2_winner: tie, judge: ('gpt-4-1106-preview', 'pair-v2')\n",
      "  6%|██▋                                       | 10/160 [03:49<48:24, 19.37s/it]question: 91, turn: 1, model_1: mistral-dolphin-dpo, model_2: mistral-dolphin-sft, g1_winner: tie, g2_winner: tie, judge: ('gpt-4-1106-preview', 'pair-v2')\n",
      "  7%|██▉                                       | 11/160 [03:51<35:06, 14.14s/it]question: 92, turn: 1, model_1: mistral-dolphin-dpo, model_2: mistral-dolphin-sft, g1_winner: tie, g2_winner: model_1, judge: ('gpt-4-1106-preview', 'pair-v2')\n",
      "  8%|███▏                                      | 12/160 [04:00<31:06, 12.61s/it]question: 93, turn: 1, model_1: mistral-dolphin-dpo, model_2: mistral-dolphin-sft, g1_winner: model_1, g2_winner: tie, judge: ('gpt-4-1106-preview', 'pair-v2')\n",
      "  8%|███▍                                      | 13/160 [04:19<35:19, 14.42s/it]question: 94, turn: 1, model_1: mistral-dolphin-dpo, model_2: mistral-dolphin-sft, g1_winner: model_1, g2_winner: tie, judge: ('gpt-4-1106-preview', 'pair-v2')\n",
      "  9%|███▋                                      | 14/160 [04:40<39:42, 16.32s/it]question: 95, turn: 1, model_1: mistral-dolphin-dpo, model_2: mistral-dolphin-sft, g1_winner: model_2, g2_winner: model_1, judge: ('gpt-4-1106-preview', 'pair-v2')\n",
      "  9%|███▊                                    | 15/160 [05:24<1:00:05, 24.86s/it]question: 96, turn: 1, model_1: mistral-dolphin-dpo, model_2: mistral-dolphin-sft, g1_winner: model_2, g2_winner: model_2, judge: ('gpt-4-1106-preview', 'pair-v2')\n",
      " 10%|████                                    | 16/160 [06:01<1:08:05, 28.37s/it]question: 97, turn: 1, model_1: mistral-dolphin-dpo, model_2: mistral-dolphin-sft, g1_winner: tie, g2_winner: tie, judge: ('gpt-4-1106-preview', 'pair-v2')\n",
      " 11%|████▎                                   | 17/160 [06:25<1:04:56, 27.25s/it]question: 98, turn: 1, model_1: mistral-dolphin-dpo, model_2: mistral-dolphin-sft, g1_winner: model_1, g2_winner: tie, judge: ('gpt-4-1106-preview', 'pair-v2')\n",
      " 11%|████▋                                     | 18/160 [06:37<53:16, 22.51s/it]question: 99, turn: 1, model_1: mistral-dolphin-dpo, model_2: mistral-dolphin-sft, g1_winner: tie, g2_winner: tie, judge: ('gpt-4-1106-preview', 'pair-v2')\n",
      " 12%|████▉                                     | 19/160 [06:38<38:01, 16.18s/it]question: 100, turn: 1, model_1: mistral-dolphin-dpo, model_2: mistral-dolphin-sft, g1_winner: tie, g2_winner: tie, judge: ('gpt-4-1106-preview', 'pair-v2')\n",
      " 12%|█████▎                                    | 20/160 [06:40<27:52, 11.94s/it]question: 131, turn: 1, model_1: mistral-dolphin-dpo, model_2: mistral-dolphin-sft, g1_winner: tie, g2_winner: tie, judge: ('gpt-4-1106-preview', 'pair-v2')\n",
      " 13%|█████▌                                    | 21/160 [07:00<33:19, 14.38s/it]question: 132, turn: 1, model_1: mistral-dolphin-dpo, model_2: mistral-dolphin-sft, g1_winner: tie, g2_winner: tie, judge: ('gpt-4-1106-preview', 'pair-v2')\n",
      " 14%|█████▊                                    | 22/160 [07:02<24:33, 10.68s/it]question: 133, turn: 1, model_1: mistral-dolphin-dpo, model_2: mistral-dolphin-sft, g1_winner: model_2, g2_winner: model_2, judge: ('gpt-4-1106-preview', 'pair-v2')\n",
      " 14%|██████                                    | 23/160 [07:04<18:06,  7.93s/it]question: 134, turn: 1, model_1: mistral-dolphin-dpo, model_2: mistral-dolphin-sft, g1_winner: model_1, g2_winner: model_1, judge: ('gpt-4-1106-preview', 'pair-v2')\n",
      " 15%|██████▎                                   | 24/160 [07:07<14:19,  6.32s/it]question: 135, turn: 1, model_1: mistral-dolphin-dpo, model_2: mistral-dolphin-sft, g1_winner: tie, g2_winner: tie, judge: ('gpt-4-1106-preview', 'pair-v2')\n",
      " 16%|██████▌                                   | 25/160 [07:09<11:36,  5.16s/it]question: 136, turn: 1, model_1: mistral-dolphin-dpo, model_2: mistral-dolphin-sft, g1_winner: tie, g2_winner: tie, judge: ('gpt-4-1106-preview', 'pair-v2')\n",
      " 16%|██████▊                                   | 26/160 [07:22<16:38,  7.45s/it]question: 137, turn: 1, model_1: mistral-dolphin-dpo, model_2: mistral-dolphin-sft, g1_winner: tie, g2_winner: tie, judge: ('gpt-4-1106-preview', 'pair-v2')\n",
      " 17%|███████                                   | 27/160 [08:03<39:02, 17.62s/it]question: 138, turn: 1, model_1: mistral-dolphin-dpo, model_2: mistral-dolphin-sft, g1_winner: model_1, g2_winner: model_1, judge: ('gpt-4-1106-preview', 'pair-v2')\n",
      " 18%|███████▎                                  | 28/160 [08:11<32:35, 14.81s/it]question: 139, turn: 1, model_1: mistral-dolphin-dpo, model_2: mistral-dolphin-sft, g1_winner: model_1, g2_winner: model_1, judge: ('gpt-4-1106-preview', 'pair-v2')\n",
      " 18%|███████▌                                  | 29/160 [08:46<45:16, 20.74s/it]question: 140, turn: 1, model_1: mistral-dolphin-dpo, model_2: mistral-dolphin-sft, g1_winner: tie, g2_winner: tie, judge: ('gpt-4-1106-preview', 'pair-v2')\n",
      " 19%|███████▉                                  | 30/160 [08:53<35:57, 16.59s/it]question: 141, turn: 1, model_1: mistral-dolphin-dpo, model_2: mistral-dolphin-sft, g1_winner: tie, g2_winner: tie, judge: ('gpt-4-1106-preview', 'pair-v2')\n",
      " 19%|████████▏                                 | 31/160 [09:21<43:20, 20.16s/it]question: 142, turn: 1, model_1: mistral-dolphin-dpo, model_2: mistral-dolphin-sft, g1_winner: tie, g2_winner: model_2, judge: ('gpt-4-1106-preview', 'pair-v2')\n",
      " 20%|████████▍                                 | 32/160 [09:46<45:34, 21.36s/it]question: 143, turn: 1, model_1: mistral-dolphin-dpo, model_2: mistral-dolphin-sft, g1_winner: model_1, g2_winner: model_1, judge: ('gpt-4-1106-preview', 'pair-v2')\n",
      " 21%|████████▎                               | 33/160 [10:31<1:00:45, 28.71s/it]question: 144, turn: 1, model_1: mistral-dolphin-dpo, model_2: mistral-dolphin-sft, g1_winner: model_1, g2_winner: model_2, judge: ('gpt-4-1106-preview', 'pair-v2')\n",
      " 21%|████████▉                                 | 34/160 [10:45<50:52, 24.23s/it]question: 145, turn: 1, model_1: mistral-dolphin-dpo, model_2: mistral-dolphin-sft, g1_winner: model_2, g2_winner: tie, judge: ('gpt-4-1106-preview', 'pair-v2')\n",
      " 22%|█████████▏                                | 35/160 [11:01<45:15, 21.73s/it]question: 146, turn: 1, model_1: mistral-dolphin-dpo, model_2: mistral-dolphin-sft, g1_winner: model_2, g2_winner: tie, judge: ('gpt-4-1106-preview', 'pair-v2')\n",
      " 22%|█████████                               | 36/160 [11:47<1:00:01, 29.05s/it]question: 147, turn: 1, model_1: mistral-dolphin-dpo, model_2: mistral-dolphin-sft, g1_winner: tie, g2_winner: tie, judge: ('gpt-4-1106-preview', 'pair-v2')\n",
      " 23%|█████████▎                              | 37/160 [12:53<1:21:57, 39.98s/it]question: 148, turn: 1, model_1: mistral-dolphin-dpo, model_2: mistral-dolphin-sft, g1_winner: model_1, g2_winner: model_1, judge: ('gpt-4-1106-preview', 'pair-v2')\n",
      " 24%|█████████▌                              | 38/160 [13:39<1:24:58, 41.79s/it]question: 149, turn: 1, model_1: mistral-dolphin-dpo, model_2: mistral-dolphin-sft, g1_winner: tie, g2_winner: tie, judge: ('gpt-4-1106-preview', 'pair-v2')\n",
      " 24%|█████████▊                              | 39/160 [14:02<1:13:14, 36.32s/it]question: 150, turn: 1, model_1: mistral-dolphin-dpo, model_2: mistral-dolphin-sft, g1_winner: tie, g2_winner: tie, judge: ('gpt-4-1106-preview', 'pair-v2')\n",
      " 25%|██████████▌                               | 40/160 [14:06<53:01, 26.51s/it]question: 151, turn: 1, model_1: mistral-dolphin-dpo, model_2: mistral-dolphin-sft, g1_winner: model_1, g2_winner: model_2, judge: ('gpt-4-1106-preview', 'pair-v2')\n",
      " 26%|██████████▊                               | 41/160 [14:30<51:15, 25.85s/it]question: 152, turn: 1, model_1: mistral-dolphin-dpo, model_2: mistral-dolphin-sft, g1_winner: model_1, g2_winner: tie, judge: ('gpt-4-1106-preview', 'pair-v2')\n",
      " 26%|██████████▌                             | 42/160 [15:35<1:13:52, 37.57s/it]question: 153, turn: 1, model_1: mistral-dolphin-dpo, model_2: mistral-dolphin-sft, g1_winner: model_1, g2_winner: tie, judge: ('gpt-4-1106-preview', 'pair-v2')\n",
      " 27%|██████████▊                             | 43/160 [16:21<1:17:56, 39.97s/it]question: 154, turn: 1, model_1: mistral-dolphin-dpo, model_2: mistral-dolphin-sft, g1_winner: model_1, g2_winner: tie, judge: ('gpt-4-1106-preview', 'pair-v2')\n",
      " 28%|███████████                             | 44/160 [17:30<1:34:26, 48.85s/it]question: 155, turn: 1, model_1: mistral-dolphin-dpo, model_2: mistral-dolphin-sft, g1_winner: tie, g2_winner: tie, judge: ('gpt-4-1106-preview', 'pair-v2')\n",
      " 28%|███████████▎                            | 45/160 [17:54<1:19:13, 41.33s/it]question: 156, turn: 1, model_1: mistral-dolphin-dpo, model_2: mistral-dolphin-sft, g1_winner: tie, g2_winner: tie, judge: ('gpt-4-1106-preview', 'pair-v2')\n",
      " 29%|████████████                              | 46/160 [17:56<55:58, 29.46s/it]question: 157, turn: 1, model_1: mistral-dolphin-dpo, model_2: mistral-dolphin-sft, g1_winner: model_1, g2_winner: tie, judge: ('gpt-4-1106-preview', 'pair-v2')\n",
      " 29%|████████████▎                             | 47/160 [18:28<57:03, 30.30s/it]question: 158, turn: 1, model_1: mistral-dolphin-dpo, model_2: mistral-dolphin-sft, g1_winner: model_1, g2_winner: model_2, judge: ('gpt-4-1106-preview', 'pair-v2')\n",
      " 30%|████████████▌                             | 48/160 [18:58<56:29, 30.26s/it]question: 159, turn: 1, model_1: mistral-dolphin-dpo, model_2: mistral-dolphin-sft, g1_winner: model_2, g2_winner: model_2, judge: ('gpt-4-1106-preview', 'pair-v2')\n",
      " 31%|████████████▎                           | 49/160 [19:40<1:02:23, 33.72s/it]question: 160, turn: 1, model_1: mistral-dolphin-dpo, model_2: mistral-dolphin-sft, g1_winner: tie, g2_winner: tie, judge: ('gpt-4-1106-preview', 'pair-v2')\n",
      " 31%|█████████████▏                            | 50/160 [20:05<56:57, 31.07s/it]question: 101, turn: 1, model_1: mistral-dolphin-dpo, model_2: mistral-dolphin-sft, g1_winner: tie, g2_winner: tie, judge: ('gpt-4-1106-preview', 'pair-math-v1')\n",
      " 32%|█████████████▍                            | 51/160 [20:31<53:42, 29.57s/it]question: 102, turn: 1, model_1: mistral-dolphin-dpo, model_2: mistral-dolphin-sft, g1_winner: model_1, g2_winner: model_2, judge: ('gpt-4-1106-preview', 'pair-math-v1')\n",
      " 32%|█████████████                           | 52/160 [21:16<1:01:29, 34.16s/it]question: 103, turn: 1, model_1: mistral-dolphin-dpo, model_2: mistral-dolphin-sft, g1_winner: tie, g2_winner: tie, judge: ('gpt-4-1106-preview', 'pair-math-v1')\n",
      " 33%|█████████████▎                          | 53/160 [22:10<1:11:47, 40.26s/it]question: 104, turn: 1, model_1: mistral-dolphin-dpo, model_2: mistral-dolphin-sft, g1_winner: tie, g2_winner: tie, judge: ('gpt-4-1106-preview', 'pair-math-v1')\n",
      " 34%|█████████████▌                          | 54/160 [22:40<1:05:43, 37.20s/it]question: 105, turn: 1, model_1: mistral-dolphin-dpo, model_2: mistral-dolphin-sft, g1_winner: model_1, g2_winner: tie, judge: ('gpt-4-1106-preview', 'pair-math-v1')\n",
      " 34%|█████████████▊                          | 55/160 [23:36<1:14:36, 42.63s/it]question: 106, turn: 1, model_1: mistral-dolphin-dpo, model_2: mistral-dolphin-sft, g1_winner: tie, g2_winner: tie, judge: ('gpt-4-1106-preview', 'pair-math-v1')\n",
      " 35%|██████████████                          | 56/160 [23:55<1:01:50, 35.68s/it]question: 107, turn: 1, model_1: mistral-dolphin-dpo, model_2: mistral-dolphin-sft, g1_winner: tie, g2_winner: tie, judge: ('gpt-4-1106-preview', 'pair-math-v1')\n",
      " 36%|██████████████▎                         | 57/160 [24:28<1:00:04, 35.00s/it]question: 108, turn: 1, model_1: mistral-dolphin-dpo, model_2: mistral-dolphin-sft, g1_winner: model_1, g2_winner: model_1, judge: ('gpt-4-1106-preview', 'pair-math-v1')\n",
      " 36%|██████████████▌                         | 58/160 [25:22<1:08:49, 40.49s/it]question: 109, turn: 1, model_1: mistral-dolphin-dpo, model_2: mistral-dolphin-sft, g1_winner: model_1, g2_winner: model_1, judge: ('gpt-4-1106-preview', 'pair-math-v1')\n",
      " 37%|██████████████▊                         | 59/160 [25:55<1:04:35, 38.37s/it]question: 110, turn: 1, model_1: mistral-dolphin-dpo, model_2: mistral-dolphin-sft, g1_winner: model_1, g2_winner: model_1, judge: ('gpt-4-1106-preview', 'pair-math-v1')\n",
      " 38%|███████████████▊                          | 60/160 [26:24<59:05, 35.46s/it]question: 111, turn: 1, model_1: mistral-dolphin-dpo, model_2: mistral-dolphin-sft, g1_winner: model_1, g2_winner: tie, judge: ('gpt-4-1106-preview', 'pair-math-v1')\n",
      " 38%|███████████████▎                        | 61/160 [27:07<1:02:22, 37.80s/it]question: 112, turn: 1, model_1: mistral-dolphin-dpo, model_2: mistral-dolphin-sft, g1_winner: tie, g2_winner: tie, judge: ('gpt-4-1106-preview', 'pair-math-v1')\n",
      " 39%|████████████████▎                         | 62/160 [27:19<49:06, 30.06s/it]question: 113, turn: 1, model_1: mistral-dolphin-dpo, model_2: mistral-dolphin-sft, g1_winner: model_2, g2_winner: model_2, judge: ('gpt-4-1106-preview', 'pair-math-v1')\n",
      " 39%|████████████████▌                         | 63/160 [27:48<48:11, 29.80s/it]question: 114, turn: 1, model_1: mistral-dolphin-dpo, model_2: mistral-dolphin-sft, g1_winner: tie, g2_winner: tie, judge: ('gpt-4-1106-preview', 'pair-math-v1')\n",
      " 40%|████████████████▊                         | 64/160 [28:17<46:58, 29.36s/it]question: 115, turn: 1, model_1: mistral-dolphin-dpo, model_2: mistral-dolphin-sft, g1_winner: tie, g2_winner: tie, judge: ('gpt-4-1106-preview', 'pair-math-v1')\n",
      " 41%|█████████████████                         | 65/160 [28:58<51:56, 32.80s/it]question: 116, turn: 1, model_1: mistral-dolphin-dpo, model_2: mistral-dolphin-sft, g1_winner: tie, g2_winner: tie, judge: ('gpt-4-1106-preview', 'pair-math-v1')\n",
      " 41%|█████████████████▎                        | 66/160 [29:44<57:52, 36.94s/it]question: 117, turn: 1, model_1: mistral-dolphin-dpo, model_2: mistral-dolphin-sft, g1_winner: tie, g2_winner: model_2, judge: ('gpt-4-1106-preview', 'pair-math-v1')\n",
      " 42%|████████████████▊                       | 67/160 [30:27<1:00:13, 38.85s/it]question: 118, turn: 1, model_1: mistral-dolphin-dpo, model_2: mistral-dolphin-sft, g1_winner: model_2, g2_winner: model_2, judge: ('gpt-4-1106-preview', 'pair-math-v1')\n",
      " 42%|█████████████████▊                        | 68/160 [30:58<55:37, 36.28s/it]question: 119, turn: 1, model_1: mistral-dolphin-dpo, model_2: mistral-dolphin-sft, g1_winner: tie, g2_winner: tie, judge: ('gpt-4-1106-preview', 'pair-math-v1')\n",
      " 43%|██████████████████                        | 69/160 [31:21<49:18, 32.51s/it]question: 120, turn: 1, model_1: mistral-dolphin-dpo, model_2: mistral-dolphin-sft, g1_winner: tie, g2_winner: tie, judge: ('gpt-4-1106-preview', 'pair-math-v1')\n",
      " 44%|██████████████████▍                       | 70/160 [31:48<46:10, 30.79s/it]question: 121, turn: 1, model_1: mistral-dolphin-dpo, model_2: mistral-dolphin-sft, g1_winner: model_2, g2_winner: model_2, judge: ('gpt-4-1106-preview', 'pair-math-v1')\n",
      " 44%|██████████████████▋                       | 71/160 [32:25<48:14, 32.52s/it]question: 122, turn: 1, model_1: mistral-dolphin-dpo, model_2: mistral-dolphin-sft, g1_winner: model_1, g2_winner: model_1, judge: ('gpt-4-1106-preview', 'pair-math-v1')\n",
      " 45%|██████████████████▉                       | 72/160 [32:51<45:06, 30.76s/it]question: 123, turn: 1, model_1: mistral-dolphin-dpo, model_2: mistral-dolphin-sft, g1_winner: tie, g2_winner: model_1, judge: ('gpt-4-1106-preview', 'pair-math-v1')\n",
      " 46%|███████████████████▏                      | 73/160 [33:31<48:23, 33.37s/it]question: 124, turn: 1, model_1: mistral-dolphin-dpo, model_2: mistral-dolphin-sft, g1_winner: tie, g2_winner: tie, judge: ('gpt-4-1106-preview', 'pair-math-v1')\n",
      " 46%|███████████████████▍                      | 74/160 [34:28<58:07, 40.55s/it]question: 125, turn: 1, model_1: mistral-dolphin-dpo, model_2: mistral-dolphin-sft, g1_winner: model_1, g2_winner: tie, judge: ('gpt-4-1106-preview', 'pair-math-v1')\n",
      " 47%|███████████████████▋                      | 75/160 [35:11<58:18, 41.16s/it]question: 126, turn: 1, model_1: mistral-dolphin-dpo, model_2: mistral-dolphin-sft, g1_winner: tie, g2_winner: tie, judge: ('gpt-4-1106-preview', 'pair-math-v1')\n",
      " 48%|███████████████████▉                      | 76/160 [35:48<55:56, 39.96s/it]question: 127, turn: 1, model_1: mistral-dolphin-dpo, model_2: mistral-dolphin-sft, g1_winner: tie, g2_winner: tie, judge: ('gpt-4-1106-preview', 'pair-math-v1')\n",
      " 48%|███████████████████▎                    | 77/160 [36:40<1:00:26, 43.70s/it]question: 128, turn: 1, model_1: mistral-dolphin-dpo, model_2: mistral-dolphin-sft, g1_winner: tie, g2_winner: tie, judge: ('gpt-4-1106-preview', 'pair-math-v1')\n",
      " 49%|███████████████████▌                    | 78/160 [37:29<1:01:49, 45.24s/it]question: 129, turn: 1, model_1: mistral-dolphin-dpo, model_2: mistral-dolphin-sft, g1_winner: tie, g2_winner: tie, judge: ('gpt-4-1106-preview', 'pair-math-v1')\n",
      " 49%|███████████████████▊                    | 79/160 [38:15<1:01:19, 45.42s/it]question: 130, turn: 1, model_1: mistral-dolphin-dpo, model_2: mistral-dolphin-sft, g1_winner: model_1, g2_winner: model_1, judge: ('gpt-4-1106-preview', 'pair-math-v1')\n",
      " 50%|████████████████████                    | 80/160 [39:12<1:05:18, 48.98s/it]question: 81, turn: 2, model_1: mistral-dolphin-dpo, model_2: mistral-dolphin-sft, g1_winner: model_2, g2_winner: tie, judge: ('gpt-4-1106-preview', 'pair-v2-multi-turn')\n",
      " 51%|█████████████████████▎                    | 81/160 [39:39<55:52, 42.43s/it]question: 82, turn: 2, model_1: mistral-dolphin-dpo, model_2: mistral-dolphin-sft, g1_winner: tie, g2_winner: tie, judge: ('gpt-4-1106-preview', 'pair-v2-multi-turn')\n",
      " 51%|█████████████████████▌                    | 82/160 [40:08<49:43, 38.26s/it]question: 83, turn: 2, model_1: mistral-dolphin-dpo, model_2: mistral-dolphin-sft, g1_winner: model_1, g2_winner: model_1, judge: ('gpt-4-1106-preview', 'pair-v2-multi-turn')\n",
      " 52%|█████████████████████▊                    | 83/160 [40:32<43:37, 33.99s/it]question: 84, turn: 2, model_1: mistral-dolphin-dpo, model_2: mistral-dolphin-sft, g1_winner: tie, g2_winner: tie, judge: ('gpt-4-1106-preview', 'pair-v2-multi-turn')\n",
      " 52%|██████████████████████                    | 84/160 [41:22<49:01, 38.70s/it]question: 85, turn: 2, model_1: mistral-dolphin-dpo, model_2: mistral-dolphin-sft, g1_winner: tie, g2_winner: tie, judge: ('gpt-4-1106-preview', 'pair-v2-multi-turn')\n",
      " 53%|██████████████████████▎                   | 85/160 [42:07<50:43, 40.58s/it]question: 86, turn: 2, model_1: mistral-dolphin-dpo, model_2: mistral-dolphin-sft, g1_winner: model_1, g2_winner: model_1, judge: ('gpt-4-1106-preview', 'pair-v2-multi-turn')\n",
      " 54%|██████████████████████▌                   | 86/160 [43:00<54:39, 44.32s/it]question: 87, turn: 2, model_1: mistral-dolphin-dpo, model_2: mistral-dolphin-sft, g1_winner: tie, g2_winner: tie, judge: ('gpt-4-1106-preview', 'pair-v2-multi-turn')\n",
      " 54%|██████████████████████▊                   | 87/160 [43:22<45:42, 37.56s/it]question: 88, turn: 2, model_1: mistral-dolphin-dpo, model_2: mistral-dolphin-sft, g1_winner: model_2, g2_winner: model_2, judge: ('gpt-4-1106-preview', 'pair-v2-multi-turn')\n",
      " 55%|███████████████████████                   | 88/160 [44:09<48:45, 40.64s/it]question: 89, turn: 2, model_1: mistral-dolphin-dpo, model_2: mistral-dolphin-sft, g1_winner: model_2, g2_winner: tie, judge: ('gpt-4-1106-preview', 'pair-v2-multi-turn')\n",
      " 56%|███████████████████████▎                  | 89/160 [44:44<45:50, 38.74s/it]question: 90, turn: 2, model_1: mistral-dolphin-dpo, model_2: mistral-dolphin-sft, g1_winner: model_1, g2_winner: tie, judge: ('gpt-4-1106-preview', 'pair-v2-multi-turn')\n",
      " 56%|███████████████████████▋                  | 90/160 [45:13<42:04, 36.06s/it]question: 91, turn: 2, model_1: mistral-dolphin-dpo, model_2: mistral-dolphin-sft, g1_winner: tie, g2_winner: tie, judge: ('gpt-4-1106-preview', 'pair-v2-multi-turn')\n",
      " 57%|███████████████████████▉                  | 91/160 [45:51<42:08, 36.65s/it]question: 92, turn: 2, model_1: mistral-dolphin-dpo, model_2: mistral-dolphin-sft, g1_winner: model_1, g2_winner: model_1, judge: ('gpt-4-1106-preview', 'pair-v2-multi-turn')\n",
      " 57%|████████████████████████▏                 | 92/160 [46:38<45:02, 39.74s/it]question: 93, turn: 2, model_1: mistral-dolphin-dpo, model_2: mistral-dolphin-sft, g1_winner: model_1, g2_winner: model_1, judge: ('gpt-4-1106-preview', 'pair-v2-multi-turn')\n",
      " 58%|████████████████████████▍                 | 93/160 [47:34<49:34, 44.40s/it]question: 94, turn: 2, model_1: mistral-dolphin-dpo, model_2: mistral-dolphin-sft, g1_winner: tie, g2_winner: model_2, judge: ('gpt-4-1106-preview', 'pair-v2-multi-turn')\n",
      " 59%|████████████████████████▋                 | 94/160 [48:25<51:01, 46.39s/it]question: 95, turn: 2, model_1: mistral-dolphin-dpo, model_2: mistral-dolphin-sft, g1_winner: model_1, g2_winner: model_1, judge: ('gpt-4-1106-preview', 'pair-v2-multi-turn')\n",
      " 59%|████████████████████████▉                 | 95/160 [49:11<50:04, 46.22s/it]question: 96, turn: 2, model_1: mistral-dolphin-dpo, model_2: mistral-dolphin-sft, g1_winner: model_1, g2_winner: model_2, judge: ('gpt-4-1106-preview', 'pair-v2-multi-turn')\n",
      " 60%|█████████████████████████▏                | 96/160 [49:37<43:03, 40.36s/it]question: 97, turn: 2, model_1: mistral-dolphin-dpo, model_2: mistral-dolphin-sft, g1_winner: model_1, g2_winner: model_2, judge: ('gpt-4-1106-preview', 'pair-v2-multi-turn')\n",
      " 61%|█████████████████████████▍                | 97/160 [50:07<38:59, 37.13s/it]question: 98, turn: 2, model_1: mistral-dolphin-dpo, model_2: mistral-dolphin-sft, g1_winner: model_1, g2_winner: model_1, judge: ('gpt-4-1106-preview', 'pair-v2-multi-turn')\n",
      " 61%|█████████████████████████▋                | 98/160 [50:39<36:56, 35.75s/it]question: 99, turn: 2, model_1: mistral-dolphin-dpo, model_2: mistral-dolphin-sft, g1_winner: tie, g2_winner: tie, judge: ('gpt-4-1106-preview', 'pair-v2-multi-turn')\n",
      " 62%|█████████████████████████▉                | 99/160 [51:34<41:59, 41.30s/it]question: 100, turn: 2, model_1: mistral-dolphin-dpo, model_2: mistral-dolphin-sft, g1_winner: model_1, g2_winner: tie, judge: ('gpt-4-1106-preview', 'pair-v2-multi-turn')\n",
      " 62%|█████████████████████████▋               | 100/160 [52:08<39:20, 39.34s/it]question: 131, turn: 2, model_1: mistral-dolphin-dpo, model_2: mistral-dolphin-sft, g1_winner: model_1, g2_winner: model_2, judge: ('gpt-4-1106-preview', 'pair-v2-multi-turn')\n",
      " 63%|█████████████████████████▉               | 101/160 [52:35<35:02, 35.63s/it]question: 132, turn: 2, model_1: mistral-dolphin-dpo, model_2: mistral-dolphin-sft, g1_winner: model_2, g2_winner: model_2, judge: ('gpt-4-1106-preview', 'pair-v2-multi-turn')\n",
      " 64%|██████████████████████████▏              | 102/160 [53:12<34:52, 36.07s/it]question: 133, turn: 2, model_1: mistral-dolphin-dpo, model_2: mistral-dolphin-sft, g1_winner: model_1, g2_winner: model_1, judge: ('gpt-4-1106-preview', 'pair-v2-multi-turn')\n",
      " 64%|██████████████████████████▍              | 103/160 [53:26<27:48, 29.28s/it]question: 134, turn: 2, model_1: mistral-dolphin-dpo, model_2: mistral-dolphin-sft, g1_winner: model_2, g2_winner: tie, judge: ('gpt-4-1106-preview', 'pair-v2-multi-turn')\n",
      " 65%|██████████████████████████▋              | 104/160 [54:00<28:43, 30.78s/it]question: 135, turn: 2, model_1: mistral-dolphin-dpo, model_2: mistral-dolphin-sft, g1_winner: model_1, g2_winner: model_1, judge: ('gpt-4-1106-preview', 'pair-v2-multi-turn')\n",
      " 66%|██████████████████████████▉              | 105/160 [54:19<24:51, 27.12s/it]question: 136, turn: 2, model_1: mistral-dolphin-dpo, model_2: mistral-dolphin-sft, g1_winner: model_1, g2_winner: model_1, judge: ('gpt-4-1106-preview', 'pair-v2-multi-turn')\n",
      " 66%|███████████████████████████▏             | 106/160 [54:49<25:16, 28.08s/it]question: 137, turn: 2, model_1: mistral-dolphin-dpo, model_2: mistral-dolphin-sft, g1_winner: model_1, g2_winner: model_2, judge: ('gpt-4-1106-preview', 'pair-v2-multi-turn')\n",
      " 67%|███████████████████████████▍             | 107/160 [55:22<26:06, 29.55s/it]question: 138, turn: 2, model_1: mistral-dolphin-dpo, model_2: mistral-dolphin-sft, g1_winner: model_2, g2_winner: model_2, judge: ('gpt-4-1106-preview', 'pair-v2-multi-turn')\n",
      " 68%|███████████████████████████▋             | 108/160 [55:55<26:34, 30.66s/it]question: 139, turn: 2, model_1: mistral-dolphin-dpo, model_2: mistral-dolphin-sft, g1_winner: model_1, g2_winner: model_1, judge: ('gpt-4-1106-preview', 'pair-v2-multi-turn')\n",
      " 68%|███████████████████████████▉             | 109/160 [56:21<24:52, 29.26s/it]question: 140, turn: 2, model_1: mistral-dolphin-dpo, model_2: mistral-dolphin-sft, g1_winner: model_1, g2_winner: tie, judge: ('gpt-4-1106-preview', 'pair-v2-multi-turn')\n",
      " 69%|████████████████████████████▏            | 110/160 [57:01<26:54, 32.28s/it]question: 141, turn: 2, model_1: mistral-dolphin-dpo, model_2: mistral-dolphin-sft, g1_winner: tie, g2_winner: tie, judge: ('gpt-4-1106-preview', 'pair-v2-multi-turn')\n",
      " 69%|████████████████████████████▍            | 111/160 [57:20<23:17, 28.53s/it]question: 142, turn: 2, model_1: mistral-dolphin-dpo, model_2: mistral-dolphin-sft, g1_winner: model_1, g2_winner: model_1, judge: ('gpt-4-1106-preview', 'pair-v2-multi-turn')\n",
      " 70%|████████████████████████████▋            | 112/160 [58:24<31:19, 39.15s/it]question: 143, turn: 2, model_1: mistral-dolphin-dpo, model_2: mistral-dolphin-sft, g1_winner: model_1, g2_winner: model_1, judge: ('gpt-4-1106-preview', 'pair-v2-multi-turn')\n",
      " 71%|████████████████████████████▉            | 113/160 [59:07<31:24, 40.09s/it]question: 144, turn: 2, model_1: mistral-dolphin-dpo, model_2: mistral-dolphin-sft, g1_winner: tie, g2_winner: model_2, judge: ('gpt-4-1106-preview', 'pair-v2-multi-turn')\n",
      " 71%|█████████████████████████████▏           | 114/160 [59:27<26:16, 34.27s/it]question: 145, turn: 2, model_1: mistral-dolphin-dpo, model_2: mistral-dolphin-sft, g1_winner: model_1, g2_winner: model_1, judge: ('gpt-4-1106-preview', 'pair-v2-multi-turn')\n",
      " 72%|████████████████████████████           | 115/160 [1:00:13<28:17, 37.72s/it]question: 146, turn: 2, model_1: mistral-dolphin-dpo, model_2: mistral-dolphin-sft, g1_winner: model_1, g2_winner: model_1, judge: ('gpt-4-1106-preview', 'pair-v2-multi-turn')\n",
      " 72%|████████████████████████████▎          | 116/160 [1:00:55<28:29, 38.86s/it]question: 147, turn: 2, model_1: mistral-dolphin-dpo, model_2: mistral-dolphin-sft, g1_winner: model_1, g2_winner: model_1, judge: ('gpt-4-1106-preview', 'pair-v2-multi-turn')\n",
      " 73%|████████████████████████████▌          | 117/160 [1:02:04<34:20, 47.92s/it]question: 148, turn: 2, model_1: mistral-dolphin-dpo, model_2: mistral-dolphin-sft, g1_winner: model_2, g2_winner: model_2, judge: ('gpt-4-1106-preview', 'pair-v2-multi-turn')\n",
      " 74%|████████████████████████████▊          | 118/160 [1:02:35<29:59, 42.85s/it]question: 149, turn: 2, model_1: mistral-dolphin-dpo, model_2: mistral-dolphin-sft, g1_winner: model_1, g2_winner: model_1, judge: ('gpt-4-1106-preview', 'pair-v2-multi-turn')\n",
      " 74%|█████████████████████████████          | 119/160 [1:03:28<31:20, 45.86s/it]question: 150, turn: 2, model_1: mistral-dolphin-dpo, model_2: mistral-dolphin-sft, g1_winner: model_1, g2_winner: tie, judge: ('gpt-4-1106-preview', 'pair-v2-multi-turn')\n",
      " 75%|█████████████████████████████▎         | 120/160 [1:04:29<33:44, 50.62s/it]question: 151, turn: 2, model_1: mistral-dolphin-dpo, model_2: mistral-dolphin-sft, g1_winner: tie, g2_winner: model_2, judge: ('gpt-4-1106-preview', 'pair-v2-multi-turn')\n",
      " 76%|█████████████████████████████▍         | 121/160 [1:04:57<28:23, 43.67s/it]question: 152, turn: 2, model_1: mistral-dolphin-dpo, model_2: mistral-dolphin-sft, g1_winner: model_1, g2_winner: tie, judge: ('gpt-4-1106-preview', 'pair-v2-multi-turn')\n",
      " 76%|█████████████████████████████▋         | 122/160 [1:05:46<28:42, 45.33s/it]question: 153, turn: 2, model_1: mistral-dolphin-dpo, model_2: mistral-dolphin-sft, g1_winner: model_2, g2_winner: model_1, judge: ('gpt-4-1106-preview', 'pair-v2-multi-turn')\n",
      " 77%|█████████████████████████████▉         | 123/160 [1:06:30<27:37, 44.81s/it]question: 154, turn: 2, model_1: mistral-dolphin-dpo, model_2: mistral-dolphin-sft, g1_winner: tie, g2_winner: tie, judge: ('gpt-4-1106-preview', 'pair-v2-multi-turn')\n",
      " 78%|██████████████████████████████▏        | 124/160 [1:07:16<27:07, 45.21s/it]question: 155, turn: 2, model_1: mistral-dolphin-dpo, model_2: mistral-dolphin-sft, g1_winner: model_1, g2_winner: model_2, judge: ('gpt-4-1106-preview', 'pair-v2-multi-turn')\n",
      " 78%|██████████████████████████████▍        | 125/160 [1:08:14<28:39, 49.12s/it]question: 156, turn: 2, model_1: mistral-dolphin-dpo, model_2: mistral-dolphin-sft, g1_winner: model_1, g2_winner: model_1, judge: ('gpt-4-1106-preview', 'pair-v2-multi-turn')\n",
      " 79%|██████████████████████████████▋        | 126/160 [1:09:22<30:59, 54.69s/it]question: 157, turn: 2, model_1: mistral-dolphin-dpo, model_2: mistral-dolphin-sft, g1_winner: model_1, g2_winner: tie, judge: ('gpt-4-1106-preview', 'pair-v2-multi-turn')\n",
      " 79%|██████████████████████████████▉        | 127/160 [1:09:52<26:00, 47.28s/it]question: 158, turn: 2, model_1: mistral-dolphin-dpo, model_2: mistral-dolphin-sft, g1_winner: model_1, g2_winner: model_2, judge: ('gpt-4-1106-preview', 'pair-v2-multi-turn')\n",
      " 80%|███████████████████████████████▏       | 128/160 [1:10:21<22:25, 42.05s/it]question: 159, turn: 2, model_1: mistral-dolphin-dpo, model_2: mistral-dolphin-sft, g1_winner: model_1, g2_winner: model_2, judge: ('gpt-4-1106-preview', 'pair-v2-multi-turn')\n",
      " 81%|███████████████████████████████▍       | 129/160 [1:11:02<21:31, 41.65s/it]question: 160, turn: 2, model_1: mistral-dolphin-dpo, model_2: mistral-dolphin-sft, g1_winner: model_1, g2_winner: model_1, judge: ('gpt-4-1106-preview', 'pair-v2-multi-turn')\n",
      " 81%|███████████████████████████████▋       | 130/160 [1:11:44<20:48, 41.62s/it]question: 101, turn: 2, model_1: mistral-dolphin-dpo, model_2: mistral-dolphin-sft, g1_winner: tie, g2_winner: tie, judge: ('gpt-4-1106-preview', 'pair-math-v1-multi-turn')\n",
      " 82%|███████████████████████████████▉       | 131/160 [1:12:04<16:59, 35.16s/it]question: 102, turn: 2, model_1: mistral-dolphin-dpo, model_2: mistral-dolphin-sft, g1_winner: tie, g2_winner: tie, judge: ('gpt-4-1106-preview', 'pair-math-v1-multi-turn')\n",
      " 82%|████████████████████████████████▏      | 132/160 [1:12:27<14:41, 31.47s/it]question: 103, turn: 2, model_1: mistral-dolphin-dpo, model_2: mistral-dolphin-sft, g1_winner: model_1, g2_winner: model_1, judge: ('gpt-4-1106-preview', 'pair-math-v1-multi-turn')\n",
      " 83%|████████████████████████████████▍      | 133/160 [1:13:21<17:11, 38.22s/it]question: 104, turn: 2, model_1: mistral-dolphin-dpo, model_2: mistral-dolphin-sft, g1_winner: tie, g2_winner: tie, judge: ('gpt-4-1106-preview', 'pair-math-v1-multi-turn')\n",
      " 84%|████████████████████████████████▋      | 134/160 [1:13:57<16:15, 37.52s/it]question: 105, turn: 2, model_1: mistral-dolphin-dpo, model_2: mistral-dolphin-sft, g1_winner: tie, g2_winner: tie, judge: ('gpt-4-1106-preview', 'pair-math-v1-multi-turn')\n",
      " 84%|████████████████████████████████▉      | 135/160 [1:14:36<15:52, 38.09s/it]question: 106, turn: 2, model_1: mistral-dolphin-dpo, model_2: mistral-dolphin-sft, g1_winner: tie, g2_winner: tie, judge: ('gpt-4-1106-preview', 'pair-math-v1-multi-turn')\n",
      " 85%|█████████████████████████████████▏     | 136/160 [1:15:04<13:58, 34.94s/it]question: 107, turn: 2, model_1: mistral-dolphin-dpo, model_2: mistral-dolphin-sft, g1_winner: model_1, g2_winner: model_1, judge: ('gpt-4-1106-preview', 'pair-math-v1-multi-turn')\n",
      " 86%|█████████████████████████████████▍     | 137/160 [1:15:26<11:57, 31.19s/it]question: 108, turn: 2, model_1: mistral-dolphin-dpo, model_2: mistral-dolphin-sft, g1_winner: model_1, g2_winner: model_1, judge: ('gpt-4-1106-preview', 'pair-math-v1-multi-turn')\n",
      " 86%|█████████████████████████████████▋     | 138/160 [1:16:04<12:12, 33.28s/it]question: 109, turn: 2, model_1: mistral-dolphin-dpo, model_2: mistral-dolphin-sft, g1_winner: tie, g2_winner: tie, judge: ('gpt-4-1106-preview', 'pair-math-v1-multi-turn')\n",
      " 87%|█████████████████████████████████▉     | 139/160 [1:16:22<10:02, 28.68s/it]question: 110, turn: 2, model_1: mistral-dolphin-dpo, model_2: mistral-dolphin-sft, g1_winner: model_1, g2_winner: tie, judge: ('gpt-4-1106-preview', 'pair-math-v1-multi-turn')\n",
      " 88%|██████████████████████████████████▏    | 140/160 [1:17:02<10:42, 32.12s/it]question: 111, turn: 2, model_1: mistral-dolphin-dpo, model_2: mistral-dolphin-sft, g1_winner: tie, g2_winner: tie, judge: ('gpt-4-1106-preview', 'pair-math-v1-multi-turn')\n",
      " 88%|██████████████████████████████████▎    | 141/160 [1:18:05<13:07, 41.45s/it]question: 112, turn: 2, model_1: mistral-dolphin-dpo, model_2: mistral-dolphin-sft, g1_winner: tie, g2_winner: tie, judge: ('gpt-4-1106-preview', 'pair-math-v1-multi-turn')\n",
      " 89%|██████████████████████████████████▌    | 142/160 [1:18:34<11:18, 37.68s/it]question: 113, turn: 2, model_1: mistral-dolphin-dpo, model_2: mistral-dolphin-sft, g1_winner: tie, g2_winner: model_1, judge: ('gpt-4-1106-preview', 'pair-math-v1-multi-turn')\n",
      " 89%|██████████████████████████████████▊    | 143/160 [1:19:01<09:46, 34.53s/it]question: 114, turn: 2, model_1: mistral-dolphin-dpo, model_2: mistral-dolphin-sft, g1_winner: tie, g2_winner: tie, judge: ('gpt-4-1106-preview', 'pair-math-v1-multi-turn')\n",
      " 90%|███████████████████████████████████    | 144/160 [1:19:34<09:04, 34.03s/it]question: 115, turn: 2, model_1: mistral-dolphin-dpo, model_2: mistral-dolphin-sft, g1_winner: tie, g2_winner: tie, judge: ('gpt-4-1106-preview', 'pair-math-v1-multi-turn')\n",
      " 91%|███████████████████████████████████▎   | 145/160 [1:19:59<07:48, 31.27s/it]question: 116, turn: 2, model_1: mistral-dolphin-dpo, model_2: mistral-dolphin-sft, g1_winner: tie, g2_winner: tie, judge: ('gpt-4-1106-preview', 'pair-math-v1-multi-turn')\n",
      " 91%|███████████████████████████████████▌   | 146/160 [1:21:30<11:29, 49.23s/it]question: 117, turn: 2, model_1: mistral-dolphin-dpo, model_2: mistral-dolphin-sft, g1_winner: tie, g2_winner: tie, judge: ('gpt-4-1106-preview', 'pair-math-v1-multi-turn')\n",
      " 92%|███████████████████████████████████▊   | 147/160 [1:22:23<10:54, 50.34s/it]question: 118, turn: 2, model_1: mistral-dolphin-dpo, model_2: mistral-dolphin-sft, g1_winner: tie, g2_winner: model_2, judge: ('gpt-4-1106-preview', 'pair-math-v1-multi-turn')\n",
      " 92%|████████████████████████████████████   | 148/160 [1:23:06<09:36, 48.06s/it]question: 119, turn: 2, model_1: mistral-dolphin-dpo, model_2: mistral-dolphin-sft, g1_winner: model_1, g2_winner: model_1, judge: ('gpt-4-1106-preview', 'pair-math-v1-multi-turn')\n",
      " 93%|████████████████████████████████████▎  | 149/160 [1:23:23<07:06, 38.73s/it]question: 120, turn: 2, model_1: mistral-dolphin-dpo, model_2: mistral-dolphin-sft, g1_winner: tie, g2_winner: tie, judge: ('gpt-4-1106-preview', 'pair-math-v1-multi-turn')\n",
      " 94%|████████████████████████████████████▌  | 150/160 [1:24:04<06:34, 39.43s/it]question: 121, turn: 2, model_1: mistral-dolphin-dpo, model_2: mistral-dolphin-sft, g1_winner: tie, g2_winner: tie, judge: ('gpt-4-1106-preview', 'pair-math-v1-multi-turn')\n",
      " 94%|████████████████████████████████████▊  | 151/160 [1:24:55<06:25, 42.87s/it]question: 122, turn: 2, model_1: mistral-dolphin-dpo, model_2: mistral-dolphin-sft, g1_winner: model_2, g2_winner: tie, judge: ('gpt-4-1106-preview', 'pair-math-v1-multi-turn')\n",
      " 95%|█████████████████████████████████████  | 152/160 [1:25:31<05:26, 40.77s/it]question: 123, turn: 2, model_1: mistral-dolphin-dpo, model_2: mistral-dolphin-sft, g1_winner: tie, g2_winner: tie, judge: ('gpt-4-1106-preview', 'pair-math-v1-multi-turn')\n",
      " 96%|█████████████████████████████████████▎ | 153/160 [1:26:01<04:22, 37.55s/it]question: 124, turn: 2, model_1: mistral-dolphin-dpo, model_2: mistral-dolphin-sft, g1_winner: tie, g2_winner: tie, judge: ('gpt-4-1106-preview', 'pair-math-v1-multi-turn')\n",
      " 96%|█████████████████████████████████████▌ | 154/160 [1:26:24<03:18, 33.14s/it]question: 125, turn: 2, model_1: mistral-dolphin-dpo, model_2: mistral-dolphin-sft, g1_winner: tie, g2_winner: tie, judge: ('gpt-4-1106-preview', 'pair-math-v1-multi-turn')\n",
      " 97%|█████████████████████████████████████▊ | 155/160 [1:27:27<03:31, 42.28s/it]question: 126, turn: 2, model_1: mistral-dolphin-dpo, model_2: mistral-dolphin-sft, g1_winner: tie, g2_winner: tie, judge: ('gpt-4-1106-preview', 'pair-math-v1-multi-turn')\n",
      " 98%|██████████████████████████████████████ | 156/160 [1:28:10<02:49, 42.45s/it]question: 127, turn: 2, model_1: mistral-dolphin-dpo, model_2: mistral-dolphin-sft, g1_winner: tie, g2_winner: model_1, judge: ('gpt-4-1106-preview', 'pair-math-v1-multi-turn')\n",
      " 98%|██████████████████████████████████████▎| 157/160 [1:28:42<01:58, 39.35s/it]question: 128, turn: 2, model_1: mistral-dolphin-dpo, model_2: mistral-dolphin-sft, g1_winner: tie, g2_winner: tie, judge: ('gpt-4-1106-preview', 'pair-math-v1-multi-turn')\n",
      " 99%|██████████████████████████████████████▌| 158/160 [1:29:24<01:20, 40.11s/it]question: 129, turn: 2, model_1: mistral-dolphin-dpo, model_2: mistral-dolphin-sft, g1_winner: tie, g2_winner: tie, judge: ('gpt-4-1106-preview', 'pair-math-v1-multi-turn')\n",
      " 99%|██████████████████████████████████████▊| 159/160 [1:29:51<00:36, 36.27s/it]question: 130, turn: 2, model_1: mistral-dolphin-dpo, model_2: mistral-dolphin-sft, g1_winner: model_2, g2_winner: tie, judge: ('gpt-4-1106-preview', 'pair-math-v1-multi-turn')\n",
      "100%|███████████████████████████████████████| 160/160 [1:30:26<00:00, 33.92s/it]\n"
     ]
    }
   ],
   "source": [
    "open_ai_key=\"\" # replace with your openai key\n",
    "\n",
    "# Pairwise comparison of the two models using OpenAI's GPT-4 Turbo\n",
    "!OPENAI_API_KEY={open_ai_key} python gen_judgment.py --model-list \"mistral-dolphin-dpo\" \"mistral-dolphin-sft\" --judge-model \"gpt-4-1106-preview\" --mode \"pairwise-all\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot and compare the results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After we have the results we can plot them and compare the win-rate of the SFT and DPO model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/bash: /home/lole4878/miniconda3/lib/libtinfo.so.6: no version information available (required by /bin/bash)\n",
      "Mode: pairwise-all\n",
      "Input file: ./data/mt_bench/model_judgment/gpt-4-1106-preview_pair.jsonl\n",
      "| model               |   win |   loss |   tie |   win_rate |   loss_rate |   win_rate_adjusted |\n",
      "|:--------------------|------:|-------:|------:|-----------:|------------:|--------------------:|\n",
      "| mistral-dolphin-dpo |    33 |     10 |   117 |    0.20625 |     0.0625  |            0.571875 |\n",
      "| mistral-dolphin-sft |    10 |     33 |   117 |    0.0625  |     0.20625 |            0.428125 |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error: no \"view\" rule for type \"image/png\" passed its test case\n",
      "       (for more information, add \"--debug=1\" on the command line)\n"
     ]
    }
   ],
   "source": [
    "# Results are saved at the following location, make sure its correct\n",
    "res = \"./data/mt_bench/model_judgment/gpt-4-1106-preview_pair.jsonl\"\n",
    "\n",
    "!python show_result.py --input-file {res} --model-list \"mistral-dolphin-dpo\" \"mistral-dolphin-sft\" --judge-model \"gpt-4-1106-preview\" --mode pairwise-all\n",
    "\n",
    "# display plot from image file\n",
    "from PIL import Image\n",
    "i = Image.open(\"win_rate_gpt-4-1106-preview.png\")\n",
    "i.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By using DPO we were able to achieve a win-rate of 0.5875 compared to 0.4125 with the SFT model. This means by applying DPO we tuned our model to generate responses, which are more aligned with what humans/AI would prefer. This is not optimal yet, but it's a good start.\n",
    "\n",
    "Since the guide is only a starting point, you should consider additional evaluation methods, e.g. human evaluation or instruction-following capabilities. This means we might not have reached the full potential of the model. You should consider training for more epochs and on a larger dataset to improve the model further."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Clean up the FastChat Repository"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we temporary cloned the FastChat repository we can now clean it up by deleting the directory.\n",
    "\n",
    "Note: If you want to keep your evaluation results you should save the model_answer and judgment directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd ../../..\n",
    "\n",
    "# delete the cloned repository\n",
    "!rm -rf FastChat"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch212",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
